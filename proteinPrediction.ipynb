{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import theano\n",
    "import gzip\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Conv1D, AveragePooling1D, MaxPooling1D, TimeDistributed, LeakyReLU, BatchNormalization, Flatten\n",
    "from tensorflow.keras import optimizers, callbacks\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate, Conv1D, BatchNormalization, Flatten\n",
    "from keras.optimizers import Adagrad, Adam\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(X_train, X_valid, classSize, pssm = False):\n",
    "\n",
    "    if not pssm:\n",
    "        return (X_train[:,:,0:21], X_train[:,:,21 : (21+classSize)], \n",
    "                X_valid[:,:,0:21], X_valid[:,:,21 : (21+classSize)])\n",
    "    else:\n",
    "        return (X_train[:,:,21+classSize:], X_train[:,:,21 : (21+classSize)],\n",
    "                X_valid[:,:,21+classSize:], X_valid[:,:,21 : (21+classSize)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSize = 19 #19\n",
    "predictionIndex = 9\n",
    "classSize = 8  # 2 or 3 \n",
    "numberOfFeatures = 50 #50 #44 #45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_window = np.load('X_train_window19Middle.npy') # load\n",
    "X_valid_window = np.load('X_valid_window19Middle.npy') # load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.GzipFile('X_train_window19Middle.npy.gz', \"r\")\n",
    "X_train_window = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.GzipFile('X_valid_window19Middle.npy.gz', \"r\")\n",
    "X_valid_window = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = get_tapped_one_dataset(X_train_window, windowSize, classSize)\n",
    "#b = get_tapped_one_dataset(X_valid_window, windowSize, classSize)\n",
    "\n",
    "x_train_final, y_train_final, x_valid_final, y_valid_final = get_split(X_train_window, X_valid_window, classSize, pssm = True)\n",
    "print(x_train_final.shape, \"training data\")\n",
    "print(y_train_final.shape, \"labels for training data\")\n",
    "print(x_valid_final.shape, \"validation data\")\n",
    "print(y_valid_final.shape, \"labels for training validation\")\n",
    "\n",
    "y_train_final = y_train_final[:,0,:]\n",
    "print(y_train_final.shape)\n",
    "\n",
    "y_valid_final = y_valid_final[:,0,:]\n",
    "print(y_valid_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv1D, AveragePooling1D, MaxPooling1D, TimeDistributed, LeakyReLU, BatchNormalization, Flatten\n",
    "from keras import optimizers, callbacks\n",
    "from keras.regularizers import l2\n",
    "# import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "nn_epochs = 15\n",
    "\n",
    "   \n",
    "LR = 0.0009 # maybe after some (10-15) epochs reduce it to 0.0008-0.0007\n",
    "drop_out = 0.1\n",
    "batch_dim = 64\n",
    "\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "input_shape = (windowSize, 21)\n",
    "\n",
    "conv1_input = Input(shape=(windowSize, 21), name='InputWindow')\n",
    "conv_1 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True)(conv1_input)\n",
    "conv_1 = BatchNormalization(name='BN1')(conv_1)\n",
    "conv_1 = Dropout(drop_out)(conv_1)\n",
    "conv_2 = Conv1D( 64, 3, strides=1, padding='same', activation='relu', use_bias=True)(conv_1)\n",
    "conv_2 = BatchNormalization(name='BN2')(conv_2)\n",
    "conv_2 = Dropout(drop_out)(conv_2)\n",
    "conv_3 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True)(conv_2)\n",
    "conv_3 = BatchNormalization(name='BN3')(conv_3)\n",
    "conv_3 = Dropout(drop_out)(conv_3)\n",
    "\n",
    "\n",
    "\n",
    "conv_4 = Conv1D( 64, 3, strides=1, padding='same', activation='relu', use_bias=True)(conv_3)\n",
    "conv_4 = BatchNormalization(name='BN4')(conv_4)\n",
    "conv_4 = Dropout(drop_out)(conv_4)\n",
    "conv_5 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True)(conv_4)\n",
    "conv_5 = BatchNormalization(name='BN5')(conv_5)\n",
    "conv_5 = Dropout(drop_out)(conv_5)\n",
    "    #conv_6 = Conv1D( 73, 3, strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_5)\n",
    "    #conv_6 = BatchNormalization(name='BN6')(conv_6)\n",
    "\n",
    "\n",
    "    #conv_7 = Conv1D( 73, 3,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg)(conv_6)\n",
    "    #conv_7 = BatchNormalization(name='BN7')(conv_7)\n",
    "    #conv_8 = Conv1D( 4, 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_7)\n",
    "    #conv_8 = BatchNormalization(name='BN8')(conv_8)\n",
    "    #conv_9 = Conv1D( 4, 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_8)\n",
    "    #conv_9 = BatchNormalization(name='BN9')(conv_9)\n",
    "\n",
    "\n",
    "flatten  = Flatten()(conv_5)\n",
    "first_dense = Dense(256, activation='relu', use_bias=True)(flatten)\n",
    "second_dense = Dense(64, activation='relu', use_bias=True)(first_dense)\n",
    "\n",
    "final_model_output = Dense(classSize, activation = 'softmax', name='softmax')(second_dense)\n",
    "\n",
    "m = Model(inputs=conv1_input, outputs=final_model_output)\n",
    "\n",
    "opt = Adam(lr=LR)\n",
    "m.compile(optimizer=opt, loss=loss,metrics=['accuracy', 'mae'])\n",
    "\n",
    "print(\"\\nHyper Parameters\\n\")\n",
    "print(\"Learning Rate: \" + str(LR))\n",
    "print(\"Drop out: \" + str(drop_out))\n",
    "print(\"Batch dim: \" + str(batch_dim))\n",
    "print(\"Number of epochs: \" + str(nn_epochs))\n",
    "#print(\"Regularizers: \" + str(w_reg.l2))\n",
    "print(\"\\nLoss: \" + loss + \"\\n\")\n",
    "m.summary()\n",
    "\n",
    "#import os\n",
    " #os.environ[\"PATH\"] += os.pathsep + 'C:/Users/Ieremie/Anaconda3/pkgs/graphviz-2.38-hfd603c8_2/Library/bin'\n",
    "\n",
    " #plot_model(m)#, to_file='model.png')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv1D, AveragePooling1D, MaxPooling1D, TimeDistributed, LeakyReLU, BatchNormalization, Flatten\n",
    "from keras import optimizers, callbacks\n",
    "from keras.regularizers import l2\n",
    "# import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "nn_epochs = 35\n",
    "LR = 0.0009 # maybe after some (10-15) epochs reduce it to 0.0008-0.0007\n",
    "drop_out = 0.3\n",
    "batch_dim = 64\n",
    "\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "conv1_input = Input(shape=(windowSize, 21), name='InputWindow')\n",
    "conv_1 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True)(conv1_input)\n",
    "conv_1 = BatchNormalization(name='BN1')(conv_1)\n",
    "conv_1 = Dropout(drop_out)(conv_1)\n",
    "\n",
    "conv_2 = Conv1D( 64, 3, strides=1, padding='same', activation='relu', use_bias=True)(conv_1)\n",
    "conv_2 = BatchNormalization(name='BN2')(conv_2)\n",
    "conv_2 = Dropout(drop_out)(conv_2)\n",
    "\n",
    "conv_3 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True)(concatenate([conv1_input, conv_2]))\n",
    "conv_3 = BatchNormalization(name='BN3')(conv_3)\n",
    "conv_3 = Dropout(drop_out)(conv_3)\n",
    "\n",
    "conv_4 = Conv1D( 64, 3, strides=1, padding='same', activation='relu', use_bias=True)(conv_3)\n",
    "conv_4 = BatchNormalization(name='BN4')(conv_4)\n",
    "conv_4 = Dropout(drop_out)(conv_4)\n",
    "\n",
    "conv_5 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True)(concatenate([conv1_input, conv_4]))\n",
    "conv_5 = BatchNormalization(name='BN5')(conv_5)\n",
    "conv_5 = Dropout(drop_out)(conv_5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "merge_final = concatenate([conv_1, conv_3, conv_5], name='Final')\n",
    "\n",
    "\n",
    "flatten  = Flatten()(merge_final)\n",
    "first_dense = Dense(128, activation='relu', use_bias=True)(flatten)\n",
    "second_dense = Dense(32, activation='relu', use_bias=True)(first_dense)\n",
    "\n",
    "final_model_output = Dense(classSize, activation = 'softmax', name='softmax')(second_dense)\n",
    "\n",
    "m = Model(inputs=conv1_input, outputs=final_model_output)\n",
    "\n",
    "opt = Adam(lr=LR)\n",
    "m.compile(optimizer=opt, loss=loss,metrics=['accuracy', 'mae'])\n",
    "\n",
    "print(\"\\nHyper Parameters\\n\")\n",
    "print(\"Learning Rate: \" + str(LR))\n",
    "print(\"Drop out: \" + str(drop_out))\n",
    "print(\"Batch dim: \" + str(batch_dim))\n",
    "print(\"Number of epochs: \" + str(nn_epochs))\n",
    "#print(\"Regularizers: \" + str(w_reg.l2))\n",
    "print(\"\\nLoss: \" + loss + \"\\n\")\n",
    "m.summary()\n",
    "\n",
    "#import os\n",
    "#os.environ[\"PATH\"] += os.pathsep + 'C:/Users/Ieremie/Anaconda3/pkgs/graphviz-2.38-hfd603c8_2/Library/bin'\n",
    "\n",
    "#plot_model(m)#, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv1D, AveragePooling1D, MaxPooling1D, TimeDistributed, LeakyReLU, BatchNormalization, Flatten\n",
    "from keras import optimizers, callbacks\n",
    "from keras.regularizers import l2\n",
    "# import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "nn_epochs = 15\n",
    "LR = 0.0009 # maybe after some (10-15) epochs reduce it to 0.0008-0.0007\n",
    "drop_out = 0.1\n",
    "batch_dim = 64\n",
    "\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "conv1_input = Input(shape=(windowSize, 21), name='InputWindow')\n",
    "conv_1 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True)(conv1_input)\n",
    "conv_1 = BatchNormalization(name='BN1')(conv_1)\n",
    "conv_1 = Dropout(drop_out)(conv_1)\n",
    "\n",
    "conv_2 = Conv1D( 64, 3, strides=1, padding='same', activation='relu', use_bias=True)(conv_1)\n",
    "conv_2 = BatchNormalization(name='BN2')(conv_2)\n",
    "conv_2 = Dropout(drop_out)(conv_2)\n",
    "\n",
    "conv_3 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True)(conv_2)\n",
    "conv_3 = BatchNormalization(name='BN3')(conv_3)\n",
    "conv_3 = Dropout(drop_out)(conv_3)\n",
    "\n",
    "conv_4 = Conv1D( 64, 3, strides=1, padding='same', activation='relu', use_bias=True)(conv_3)\n",
    "conv_4 = BatchNormalization(name='BN4')(conv_4)\n",
    "conv_4 = Dropout(drop_out)(conv_4)\n",
    "\n",
    "conv_5 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True)(conv_4)\n",
    "conv_5 = BatchNormalization(name='BN5')(conv_5)\n",
    "conv_5 = Dropout(drop_out)(conv_5)\n",
    "\n",
    "\n",
    "input_for_second = concatenate([conv1_input, conv_5], name='Network1-and-input')\n",
    "\n",
    "\n",
    "\n",
    "conv_6 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True)(input_for_second)\n",
    "conv_6 = BatchNormalization(name='BN6')(conv_6)\n",
    "conv_6 = Dropout(drop_out)(conv_6)\n",
    "\n",
    "conv_7 = Conv1D( 64, 3, strides=1, padding='same', activation='relu', use_bias=True)(conv_6)\n",
    "conv_7 = BatchNormalization(name='BN7')(conv_7)\n",
    "conv_7 = Dropout(drop_out)(conv_7)\n",
    "\n",
    "conv_8 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True)(conv_7)\n",
    "conv_8 = BatchNormalization(name='BN8')(conv_8)\n",
    "conv_8 = Dropout(drop_out)(conv_8)\n",
    "\n",
    "conv_9 = Conv1D( 64, 3, strides=1, padding='same', activation='relu', use_bias=True)(conv_8)\n",
    "conv_9 = BatchNormalization(name='BN9')(conv_9)\n",
    "conv_9 = Dropout(drop_out)(conv_9)\n",
    "\n",
    "conv_10 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True)(conv_9)\n",
    "conv_10 = BatchNormalization(name='BN10')(conv_10)\n",
    "conv_10 = Dropout(drop_out)(conv_10)\n",
    "\n",
    "\n",
    "input_for_third = concatenate([conv1_input, conv_5, conv_10],name='Network1-Network2-and-input')\n",
    "\n",
    "\n",
    "\n",
    "conv_11 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True)(input_for_third)\n",
    "conv_11 = BatchNormalization(name='BN11')(conv_11)\n",
    "conv_11 = Dropout(drop_out)(conv_11)\n",
    "\n",
    "conv_12 = Conv1D( 64, 3, strides=1, padding='same', activation='relu', use_bias=True)(conv_11)\n",
    "conv_12 = BatchNormalization(name='BN12')(conv_12)\n",
    "conv_12 = Dropout(drop_out)(conv_12)\n",
    "\n",
    "conv_13 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True)(conv_12)\n",
    "conv_13 = BatchNormalization(name='BN13')(conv_13)\n",
    "conv_14 = Dropout(drop_out)(conv_13)\n",
    "\n",
    "conv_14 = Conv1D( 64, 3, strides=1, padding='same', activation='relu', use_bias=True)(conv_13)\n",
    "conv_14 = BatchNormalization(name='BN14')(conv_14)\n",
    "conv_14 = Dropout(drop_out)(conv_14)\n",
    "\n",
    "conv_15 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True)(conv_14)\n",
    "conv_15 = BatchNormalization(name='BN15')(conv_15)\n",
    "conv_15 = Dropout(drop_out)(conv_15)\n",
    "\n",
    "\n",
    "\n",
    "merge_final = concatenate([conv_5, conv_10, conv_15], name='Final')\n",
    "\n",
    "\n",
    "flatten  = Flatten()(merge_final)\n",
    "first_dense = Dense(256, activation='relu', use_bias=True)(flatten)\n",
    "second_dense = Dense(64, activation='relu', use_bias=True)(first_dense)\n",
    "\n",
    "final_model_output = Dense(classSize, activation = 'softmax', name='softmax')(second_dense)\n",
    "\n",
    "m = Model(inputs=conv1_input, outputs=final_model_output)\n",
    "\n",
    "opt = Adam(lr=LR)\n",
    "m.compile(optimizer=opt, loss=loss,metrics=['accuracy', 'mae'])\n",
    "\n",
    "print(\"\\nHyper Parameters\\n\")\n",
    "print(\"Learning Rate: \" + str(LR))\n",
    "print(\"Drop out: \" + str(drop_out))\n",
    "print(\"Batch dim: \" + str(batch_dim))\n",
    "print(\"Number of epochs: \" + str(nn_epochs))\n",
    "#print(\"Regularizers: \" + str(w_reg.l2))\n",
    "print(\"\\nLoss: \" + loss + \"\\n\")\n",
    "m.summary()\n",
    "\n",
    "#import os\n",
    "#os.environ[\"PATH\"] += os.pathsep + 'C:/Users/Ieremie/Anaconda3/pkgs/graphviz-2.38-hfd603c8_2/Library/bin'\n",
    "\n",
    "plot_model(m)#, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max', period=1)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "start_time = timer()\n",
    "history = m.fit(x_train_final, y_train_final, epochs=nn_epochs, batch_size=batch_dim,\n",
    "                validation_data=(x_valid_final, y_valid_final) ,shuffle=True,  callbacks=callbacks_list)\n",
    "\n",
    "end_time = timer()\n",
    "print(\"\\n\\nTime elapsed: \" + \"{0:.2f}\".format((end_time - start_time)) + \" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "accuracyName = 'accuracyMiddlewindowQ2W19Model2.png'\n",
    "lossName = 'lossMiddlewindowQ2W19Model2.png'\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "#plt.savefig(accuracyName)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "#plt.savefig(lossName)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.GzipFile('cb513_window19Q8.npy.gz', \"r\")\n",
    "X_test_window = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = get_tapped_one_dataset(X_test_window, windowSize, classSize)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_final = X_test_window[:,:,(21+classSize):]\n",
    "#x_test_final = X_test_window[:,:,0:21]\n",
    "y_test_final = X_test_window[:,:,21: (21+classSize)]\n",
    "print(x_test_final.shape)\n",
    "print(y_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_final = np.reshape(y_test_final, (80119,19*8))\n",
    "print(y_test_final.shape)\n",
    "y_test_final = y_test_final[:,0:8]\n",
    "print(y_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = m.evaluate(x_test_final, y_test_final)\n",
    "print(\"Loss: \" + str(scores[0]) + \", Accuracy: \" + str(scores[1]) + \", MAE: \" + str(scores[2]))\n",
    "print(\"yes boi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 3\n",
    "m.save('model_' + str(fold) + '.h5')  # creates a HDF5 file \n",
    "with open('model_scores_' + str(fold), 'wb') as file_pi:\n",
    "    pickle.dump(scores, file_pi)\n",
    "with open('model_history_' + str(fold), 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
