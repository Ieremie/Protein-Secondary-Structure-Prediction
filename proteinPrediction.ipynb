{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "#import theano\n",
    "import gzip\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Conv1D, AveragePooling1D, MaxPooling1D, TimeDistributed, LeakyReLU, BatchNormalization, Flatten\n",
    "from tensorflow.keras import optimizers, callbacks\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate, Conv1D, BatchNormalization, Flatten\n",
    "from keras.optimizers import Adagrad, Adam\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.utils import plot_model\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSize = 19 #19\n",
    "predictionIndex = 9\n",
    "classSize = 8  # 2 or 3 \n",
    "numberOfFeatures = 50 #50 #44 #45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datset_Casp(caspUpTo):\n",
    "\n",
    "    filename = \"casp12/proteinNet-30thinning-all-windows-19-middle-repeating-left-withTranskaya\"\n",
    "    casp = np.memmap(filename, dtype='float64', mode='r', shape=(4355098 + 1129862, 19, 50))\n",
    "    print(casp.shape)\n",
    "    \n",
    "    f = gzip.GzipFile('X_valid_window19Middle-repeating-left-right0.npy.gz', \"r\")\n",
    "    X_valid_window  = np.load(f)\n",
    "    print(X_valid_window.shape)\n",
    "\n",
    "    print(casp[:,:,29:].shape, \"training data\")\n",
    "    print(casp[:,0,21:29].shape, \"labels for training data\")\n",
    "    print(X_valid_window[:,:,35:56].shape, \"validation data\")\n",
    "    print(X_valid_window[:,0,22:30].shape, \"labels for training validation\")\n",
    "    \n",
    "    return casp[:caspUpTo,:,29:],casp[:caspUpTo,0,21:29],X_valid_window[:,:,35:56],X_valid_window[:,0,22:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = get_datset_Casp(1129862)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"casp12/proteinNet-30thinning-all-windows-19-middle-repeating-left-withTranskaya\"\n",
    "casp = np.memmap(filename, dtype='float64', mode='r', shape=(4355098 + 1129862, 19, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02145729 0.01365366 0.0054863  0.01212843 0.99848372 0.0066267\n",
      " 0.2748805  0.0765622  0.01448573 0.11815697 0.09708863 0.01261716\n",
      " 0.00570892 0.01852383 0.01763634 0.02104135 0.02484354 0.05625294\n",
      " 0.51499552 0.26894143 0.99938297]\n"
     ]
    }
   ],
   "source": [
    "print(casp[0,0,29:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    \n",
    "    f = gzip.GzipFile('X_train_window19Middle-repeating-left-right0.npy.gz', \"r\")\n",
    "    X_train_window = np.load(f)\n",
    "    \n",
    "    f = gzip.GzipFile('X_valid_window19Middle-repeating-left-right0.npy.gz', \"r\")\n",
    "    X_valid_window  = np.load(f)\n",
    "    \n",
    "    X_train_window = np.concatenate((X_train_window, X_valid_window), axis = 0)\n",
    "    print(X_train_window.shape)\n",
    "    \n",
    "    \n",
    "    X_train_window, X_valid_window = train_test_split(X_train_window, test_size=0.1)\n",
    "    #x_train_final, y_train_final, x_valid_final, y_valid_final = get_split(X_train_window, X_valid_window, classSize, pssm = True)\n",
    "\n",
    "    del X_valid_window\n",
    "    f = gzip.GzipFile('cb513-window19Middle-repeating-left-right0.npy.gz', \"r\")\n",
    "    X_valid_window  = np.load(f)\n",
    "    \n",
    "    x_train_final = X_train_window[:,:,35:56]\n",
    "    y_train_final = X_train_window[:,:,22:30]\n",
    "\n",
    "    x_valid_final = X_valid_window[:,:,35:56]\n",
    "    y_valid_final = X_valid_window[:,:,22:30]\n",
    "\n",
    "    print(x_train_final.shape, \"training data\")\n",
    "    print(y_train_final.shape, \"labels for training data\")\n",
    "    print(x_valid_final.shape, \"validation data\")\n",
    "    print(y_valid_final.shape, \"labels for training validation\")\n",
    "\n",
    "    y_train_final = y_train_final[:,0,:]\n",
    "    print(y_train_final.shape)\n",
    "\n",
    "    y_valid_final = y_valid_final[:,0,:]\n",
    "    print(y_valid_final.shape)\n",
    "    \n",
    "    return x_train_final,y_train_final,x_valid_final,y_valid_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv1D, AveragePooling1D, MaxPooling1D, TimeDistributed, LeakyReLU, BatchNormalization, Flatten\n",
    "from keras import optimizers, callbacks\n",
    "from keras.regularizers import l2\n",
    "# import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "def get_model():\n",
    "   \n",
    "    LR = 0.0005\n",
    "    drop_out = 0.3\n",
    "    batch_dim = 64\n",
    "    nn_epochs = 5\n",
    "    w_reg = regularizers.l2(0.1)\n",
    "    number_filters = 16\n",
    "\n",
    "    loss = 'categorical_crossentropy'\n",
    "\n",
    "\n",
    "    input_shape = (windowSize, 21)\n",
    "\n",
    "    conv1_input = Input(shape=(windowSize, 21), name='InputWindow')\n",
    "\n",
    "    conv_1 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg)(conv1_input)\n",
    "    conv_1 = BatchNormalization(name='BN1')(conv_1)\n",
    "    conv_1 = Dropout(drop_out)(conv_1)\n",
    "   # conv_2 = Conv1D( 64, 3, strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_1)\n",
    "   # conv_2 = BatchNormalization(name='BN2')(conv_2)\n",
    "   # conv_3 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_2)\n",
    "   # conv_3 = BatchNormalization(name='BN3')(conv_3)\n",
    "\n",
    "\n",
    "\n",
    "    #conv_4 = Conv1D( 64, 3, strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg)(conv_3)\n",
    "    #conv_4 = BatchNormalization(name='BN4')(conv_4)\n",
    "    #conv_5 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_4)\n",
    "    #conv_5 = BatchNormalization(name='BN5')(conv_5)\n",
    "    #conv_6 = Conv1D( 73, 3, strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_5)\n",
    "    #conv_6 = BatchNormalization(name='BN6')(conv_6)\n",
    "\n",
    "\n",
    "    #conv_7 = Conv1D( 73, 3,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg)(conv_6)\n",
    "    #conv_7 = BatchNormalization(name='BN7')(conv_7)\n",
    "    #conv_8 = Conv1D( 4, 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_7)\n",
    "    #conv_8 = BatchNormalization(name='BN8')(conv_8)\n",
    "    #conv_9 = Conv1D( 4, 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_8)\n",
    "    #conv_9 = BatchNormalization(name='BN9')(conv_9)\n",
    "\n",
    "\n",
    "    flatten  = Flatten()(conv_1)\n",
    "    first_dense = Dense(16, activation='relu', use_bias=True,  kernel_regularizer=w_reg, name='last')(flatten)\n",
    "    first_dense = BatchNormalization(name='BN10')(first_dense)\n",
    "    final_model_output = Dense(classSize, activation = 'softmax', name='softmax')(first_dense)\n",
    "\n",
    "    m = Model(inputs=conv1_input, outputs=final_model_output)\n",
    "\n",
    "    opt = Adam(lr=LR)\n",
    "    m.compile(optimizer=opt, loss=loss,metrics=['accuracy', 'mae'])\n",
    "\n",
    "    print(\"\\nHyper Parameters\\n\")\n",
    "    print(\"Learning Rate: \" + str(LR))\n",
    "    print(\"Drop out: \" + str(drop_out))\n",
    "    print(\"Batch dim: \" + str(batch_dim))\n",
    "    print(\"Number of epochs: \" + str(nn_epochs))\n",
    "    print(\"Regularizers: \" + str(w_reg.l2))\n",
    "    print(\"\\nLoss: \" + loss + \"\\n\")\n",
    "    m.summary()\n",
    "\n",
    "    #import os\n",
    "    #os.environ[\"PATH\"] += os.pathsep + 'C:/Users/Ieremie/Anaconda3/pkgs/graphviz-2.38-hfd603c8_2/Library/bin'\n",
    "\n",
    "    #plot_model(m)#, to_file='model.png')\n",
    "    return m\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv1D, AveragePooling1D, MaxPooling1D, TimeDistributed, LeakyReLU, BatchNormalization, Flatten\n",
    "from keras import optimizers, callbacks\n",
    "from keras.regularizers import l2\n",
    "# import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_model():\n",
    "\n",
    "\n",
    "    LR = 0.0009 # maybe after some (10-15) epochs reduce it to 0.0008-0.0007\n",
    "    drop_out = 0.6\n",
    "    #nn_epochs = 20\n",
    "    w_reg = regularizers.l2(0) # 0.0003)\n",
    "    number_filters = 16\n",
    "\n",
    "    loss = 'categorical_crossentropy'\n",
    "\n",
    "\n",
    "\n",
    "    input_shape = (windowSize, 21)\n",
    "\n",
    "    conv1_input = Input(shape=(windowSize, 21), name='InputWindow')\n",
    "\n",
    "    conv_1 = Conv1D( 64 , 19,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network1-filter1')(conv1_input)\n",
    "    conv_1 = BatchNormalization(name='BN1')(conv_1)\n",
    "    conv_1 = Dropout(drop_out)(conv_1)\n",
    "    conv_2 = Conv1D( 64 , 11,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network1-filter2')(conv1_input)\n",
    "    conv_2 = BatchNormalization(name='BN2')(conv_2)\n",
    "    conv_2 = Dropout(drop_out)(conv_2)\n",
    "    conv_3 = Conv1D( 64 , 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network1-filter3')(conv1_input)\n",
    "    conv_3 = BatchNormalization(name='BN3')(conv_3)\n",
    "    conv_3 = Dropout(drop_out)(conv_3)\n",
    "\n",
    "    merge_1 = concatenate([conv_1, conv_2, conv_3], name='Network1')\n",
    "    input_for_second = concatenate([conv1_input, merge_1], name='Network1-and-input')\n",
    "\n",
    "\n",
    "\n",
    "    conv_4 = Conv1D( 64 , 19,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network2-filter1')(input_for_second)\n",
    "    conv_4 = BatchNormalization(name='BN4')(conv_4)\n",
    "    conv_4 = Dropout(drop_out)(conv_4)\n",
    "    conv_5 = Conv1D( 64 , 11,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network2-filter2')(input_for_second)\n",
    "    conv_5 = BatchNormalization(name='BN5')(conv_5)\n",
    "    conv_5 = Dropout(drop_out)(conv_5)\n",
    "    conv_6 = Conv1D( 64 , 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network2-filter3')(input_for_second)\n",
    "    conv_6 = BatchNormalization(name='BN6')(conv_6)\n",
    "    conv_6 = Dropout(drop_out)(conv_6)\n",
    "\n",
    "    merge_2 = concatenate([conv_4, conv_5, conv_6], name='Network2')\n",
    "    input_for_third = concatenate([conv1_input, merge_1, merge_2],name='Network1-Network2-and-input')\n",
    "\n",
    "\n",
    "\n",
    "    conv_7 = Conv1D( 64 , 19,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network3-filter1')(input_for_third)\n",
    "    conv_7 = BatchNormalization(name='BN7')(conv_7)\n",
    "    conv_7 = Dropout(drop_out)(conv_7)\n",
    "    conv_8 = Conv1D( 64 , 11,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network3-filter2')(input_for_third)\n",
    "    conv_8 = BatchNormalization(name='BN8')(conv_8)\n",
    "    conv_8 = Dropout(drop_out)(conv_8)\n",
    "    conv_9 = Conv1D( 64 , 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network3-filter3')(input_for_third)\n",
    "    conv_9 = BatchNormalization(name='BN9')(conv_9)\n",
    "    conv_9 = Dropout(drop_out)(conv_9)\n",
    "\n",
    "    merge_3 = concatenate([conv_7, conv_8, conv_9],name='Network3')\n",
    "    input_for_4 = concatenate([conv1_input, merge_1, merge_2, merge_3],name='Network123-and-input')\n",
    "\n",
    "\n",
    "\n",
    "    conv_10 = Conv1D( 64 , 19,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network4-filter1')(input_for_4)\n",
    "    conv_10 = BatchNormalization(name='BN10')(conv_10)\n",
    "    conv_10 = Dropout(drop_out)(conv_10)\n",
    "    conv_11 = Conv1D( 64 , 11,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network4-filter2')(input_for_4)\n",
    "    conv_11 = BatchNormalization(name='BN11')(conv_11)\n",
    "    conv_11 = Dropout(drop_out)(conv_11)\n",
    "    conv_12 = Conv1D( 64 , 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network4-filter3')(input_for_4)\n",
    "    conv_12 = BatchNormalization(name='BN12')(conv_12)\n",
    "    conv_12 = Dropout(drop_out)(conv_12)\n",
    "\n",
    "    merge_4 = concatenate([conv_10, conv_11, conv_12],name='Network4')\n",
    "    input_for_5 = concatenate([conv1_input, merge_1, merge_2, merge_3, merge_4],name='Network1234-and-input')\n",
    "\n",
    "\n",
    "\n",
    "    conv_13 = Conv1D( 64 , 19,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network5-filter1')(input_for_5)\n",
    "    conv_13 = BatchNormalization(name='BN13')(conv_13)\n",
    "    conv_13 = Dropout(drop_out)(conv_13)\n",
    "    conv_14 = Conv1D( 64 , 11,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network5-filter2')(input_for_5)\n",
    "    conv_14 = BatchNormalization(name='BN14')(conv_14)\n",
    "    conv_14 = Dropout(drop_out)(conv_14)\n",
    "    conv_15 = Conv1D( 64 , 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network5-filter3')(input_for_5)\n",
    "    conv_15 = BatchNormalization(name='BN15')(conv_15)\n",
    "    conv_15 = Dropout(drop_out)(conv_15)\n",
    "\n",
    "    merge_5 = concatenate([conv_13, conv_14, conv_15],name='Network5')\n",
    "\n",
    "\n",
    "\n",
    "    merge_final = concatenate([merge_1, merge_2, merge_3,merge_4, merge_5], name='Final')\n",
    "\n",
    "\n",
    "\n",
    "    flatten  = Flatten()(merge_final)\n",
    "    first_dense = Dense(128, activation='relu', use_bias=True,  kernel_regularizer=w_reg, name='last')(flatten)\n",
    "    first_dense = BatchNormalization(name='BN16')(first_dense)\n",
    "\n",
    "    #second_dense = Dense(32, activation='relu', use_bias=True,  kernel_regularizer=w_reg, name='last')(flatten)\n",
    "    #second_dense = BatchNormalization(name='BN16')(second_dense)\n",
    "\n",
    "    final_model_output = Dense(classSize, activation = 'softmax', name='softmax')(first_dense)\n",
    "\n",
    "    m = Model(inputs=conv1_input, outputs=final_model_output)\n",
    "\n",
    "    opt = Adam(lr=LR)\n",
    "    m.compile(optimizer=opt, loss=loss,metrics=['accuracy', 'mae'])\n",
    "\n",
    "    print(\"\\nHyper Parameters\\n\")\n",
    "    print(\"Learning Rate: \" + str(LR))\n",
    "    print(\"Drop out: \" + str(drop_out))\n",
    "    print(\"Batch dim: \" + str(batch_dim))\n",
    "    print(\"Number of epochs: \" + str(nn_epochs))\n",
    "    print(\"Regularizers: \" + str(w_reg.l2))\n",
    "    print(\"\\nLoss: \" + loss + \"\\n\")\n",
    "    m.summary()\n",
    "\n",
    "    #import os\n",
    "    #os.environ[\"PATH\"] += os.pathsep + 'C:/Users/Ieremie/Anaconda3/pkgs/graphviz-2.38-hfd603c8_2/Library/bin'\n",
    "    #from keras.utils import plot_model\n",
    "    #plot_model(m)#, to_file='model.png')\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5484960, 19, 50)\n",
      "(53456, 19, 57)\n",
      "(5484960, 19, 21) training data\n",
      "(5484960, 8) labels for training data\n",
      "(53456, 19, 21) validation data\n",
      "(53456, 8) labels for training validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0505 20:35:12.962791 31872 deprecation.py:506] From C:\\Apps\\Anaconda3\\envs\\gpu-cuda10\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper Parameters\n",
      "\n",
      "Learning Rate: 0.0005\n",
      "Drop out: 0.3\n",
      "Batch dim: 64\n",
      "Number of epochs: 5\n",
      "Regularizers: 0.1\n",
      "\n",
      "Loss: categorical_crossentropy\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "InputWindow (InputLayer)     (None, 19, 21)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 19, 64)            4096      \n",
      "_________________________________________________________________\n",
      "BN1 (BatchNormalization)     (None, 19, 64)            256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 19, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1216)              0         \n",
      "_________________________________________________________________\n",
      "last (Dense)                 (None, 16)                19472     \n",
      "_________________________________________________________________\n",
      "BN10 (BatchNormalization)    (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 8)                 136       \n",
      "=================================================================\n",
      "Total params: 24,024\n",
      "Trainable params: 23,864\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Train on 2259724 samples, validate on 53456 samples\n",
      "Epoch 1/45\n",
      "2259724/2259724 [==============================] - 1627s 720us/step - loss: 0.8624 - acc: 0.6983 - mean_absolute_error: 0.0995 - val_loss: 1.1082 - val_acc: 0.6262 - val_mean_absolute_error: 0.1315\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.62616, saving model to //filestore.soton.ac.uk/users/ii1g17/casp30thining-45epochs-x2-0.hdf5\n",
      "Epoch 2/45\n",
      "1321216/2259724 [================>.............] - ETA: 10:14 - loss: 0.8155 - acc: 0.7014 - mean_absolute_error: 0.0982"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from time import time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "nr_models_to_train = 1\n",
    "nn_epochs = 45\n",
    "batch_dim = 64\n",
    "\n",
    "for i in range(0,nr_models_to_train):\n",
    "    \n",
    "    \n",
    "    x_train_final,y_train_final,x_valid_final,y_valid_final = get_datset_Casp(1129862 * 2)\n",
    "    \n",
    "    #callbacks\n",
    "    filepath=\"//filestore.soton.ac.uk/users/ii1g17/casp30thining-45epochs-x2-\" + str(i) + \".hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max', period=1)\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    m = get_model()\n",
    "\n",
    "    #training \n",
    "    start_time = timer()\n",
    "    history = m.fit(x_train_final, y_train_final, epochs=nn_epochs, batch_size=batch_dim,\n",
    "                validation_data=(x_valid_final, y_valid_final) ,shuffle=True,  callbacks=callbacks_list)\n",
    "\n",
    "    end_time = timer()\n",
    "    print(\"\\n\\nTime elapsed: \" + \"{0:.2f}\".format((end_time - start_time)) + \" s\")\n",
    "    \n",
    "    \n",
    "    #saving results\n",
    "    with open('//filestore.soton.ac.uk/users/ii1g17/casp30thining-45epochs-x2-hist' + str(i), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "accuracyName = 'accuracyMiddlewindowQ2W19Model2.png'\n",
    "lossName = 'lossMiddlewindowQ2W19Model2.png'\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "#plt.savefig(accuracyName)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "#plt.savefig(lossName)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.GzipFile('cb513_window19Q8.npy.gz', \"r\")\n",
    "X_test_window = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_final = X_test_window[:,:,(21+classSize):]\n",
    "#x_test_final = X_test_window[:,:,0:21]\n",
    "y_test_final = X_test_window[:,:,21: (21+classSize)]\n",
    "print(x_test_final.shape)\n",
    "print(y_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_final = np.reshape(y_test_final, (80119,19*8))\n",
    "print(y_test_final.shape)\n",
    "y_test_final = y_test_final[:,0:8]\n",
    "print(y_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test_final, y_test_final)\n",
    "print(\"Loss: \" + str(scores[0]) + \", Accuracy: \" + str(scores[1]) + \", MAE: \" + str(scores[2]))\n",
    "print(\"yes boi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 3\n",
    "m.save('model_' + str(fold) + '.h5')  # creates a HDF5 file \n",
    "with open('model_scores_' + str(fold), 'wb') as file_pi:\n",
    "    pickle.dump(scores, file_pi)\n",
    "with open('model_history_' + str(fold), 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
