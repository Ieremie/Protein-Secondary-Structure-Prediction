{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'secondary_proteins_prediction/data/cullpdb+profile_6133_filtered.npy.gz'\n",
    "TEST_PATH =  'secondary_proteins_prediction/data/cb513+profile_split1.npy.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gz(path):  # load a .npy.gz file\n",
    "    if path.endswith(\".gz\"):\n",
    "        f = open(path, 'rb')\n",
    "        return np.load(f)\n",
    "    else:\n",
    "        return np.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train(path=TRAIN_PATH):\n",
    "    if not os.path.isfile(path):\n",
    "        print(\"Train path is not downloaded ...\")\n",
    "        subprocess.call(\"./download_train.sh\", shell=True)\n",
    "    else:\n",
    "        print(\"Train path is downloaded ...\")\n",
    "    print(\"Loading train data ...\")\n",
    "    X_in = load_gz(path)\n",
    "    X = np.reshape(X_in, (5534, 700, 57))\n",
    "    del X_in\n",
    "    X = X[:, :, :].astype(theano.config.floatX)\n",
    "  \n",
    "    seq_names = np.arange(0, np.size(X, 0))\n",
    "\n",
    "    #X_train = X[seq_names[0:5278]]\n",
    "    X_train = X[seq_names[0:5534]]\n",
    "    X_valid = X[seq_names[5278:5534]]\n",
    "    \n",
    "    return X_train, X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train path is downloaded ...\n",
      "Loading train data ...\n",
      "Done loading train\n",
      "(5534, 700, 57)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid= get_train(TRAIN_PATH)\n",
    "print(\"Done loading train\")\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test(path=TEST_PATH):\n",
    "    if not os.path.isfile(path):\n",
    "        subprocess.call(\"./download_test.sh\", shell=True)\n",
    "    print(\"Loading test data ...\")\n",
    "    X_test_in = load_gz(path)\n",
    "    X_test = np.reshape(X_test_in, (514, 700, 57))\n",
    "    del X_test_in\n",
    "    X_test = X_test[:, :, :].astype(theano.config.floatX)\n",
    "\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data ...\n",
      "Done loading test\n",
      "(514, 700, 57)\n"
     ]
    }
   ],
   "source": [
    "X_test = get_test(TEST_PATH)\n",
    "print(\"Done loading test\")\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do: Make 1 hot encoded class - from Q8 to Q3\n",
    "       #Reshape the dataset so it has 700*windowSize seq that map to a class\n",
    "       #Look inside lasagne to see how we disregard the padding\n",
    "##\n",
    "\n",
    "def q8ClassToQ3(q8Labels):\n",
    "    \n",
    "    q3 = np.zeros(3)\n",
    "    index = np.argmax(q8Labels)\n",
    "    \n",
    "    #Helix\n",
    "    if index == 5 or index == 3 or index == 4 : # H ,G, I\n",
    "        q3[0] = 1   \n",
    "    #beta    \n",
    "    if index == 1 or index == 2: # B, E\n",
    "        q3[1] = 1    \n",
    "    #coil    \n",
    "    if index == 7 or index == 6 or index == 0 : # T, S, L\n",
    "        q3[2] = 1\n",
    "    return q3\n",
    "\n",
    "def q8ClassToQ2(q8Labels):\n",
    "    \n",
    "    q2 = np.zeros(2)\n",
    "    index = np.argmax(q8Labels)\n",
    "    #Helix\n",
    "    if index == 5: # or index == 3 or index == 4 : # H ,G, I\n",
    "        q2[0] = 1\n",
    "    else:\n",
    "        q2[1] = 1  \n",
    "    return q2\n",
    "\n",
    "def q8toQ8(q8Labels):\n",
    "    return q8Labels\n",
    "\n",
    "def changeQ8Class(dataSet, reductionFunction, numberOfFeatures):\n",
    "\n",
    "    num_seqs = np.size(dataSet, 0)\n",
    "    seqlen = np.size(dataSet, 1)\n",
    "    labels_new = np.zeros((num_seqs, seqlen, numberOfFeatures))\n",
    "\n",
    "    for i in range(np.size(dataSet, axis=0)):\n",
    "        for j in range(np.size(dataSet, axis=1)):\n",
    "            oneHot = reductionFunction(dataSet[i, j, 22:30])\n",
    "            features = np.concatenate((dataSet[i, j, 0:21], oneHot), axis=None)\n",
    "            features = np.concatenate((features, dataSet[i, j, 35:56]), axis=None)\n",
    "            labels_new[i][j] = features\n",
    "    return labels_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swapClassLabel(features, classLabel, classSize):\n",
    "\n",
    "    res = np.concatenate((features[0:21], classLabel), axis=None)\n",
    "    res = np.concatenate((res, features[ (21+classSize) :]), axis=None)\n",
    "  \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#num_classes should be q8, q3 or maybe q2?\n",
    "def proteinSequenceToWindowSequence(windowSize, predictionIndex , dataSet, classSize):\n",
    "   \n",
    "    num_seqs = np.size(dataSet, 0)\n",
    "    seqlen = np.size(dataSet, 1)\n",
    "    features = np.size(dataSet, 2)\n",
    "    dataSet_new = np.zeros((num_seqs, seqlen - windowSize + 1, windowSize, features))\n",
    "    \n",
    "    for i in range(np.size(dataSet, axis=0)):\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        for j in range(np.size(dataSet, axis=1) - windowSize):\n",
    "            classLabel = dataSet[i][j + predictionIndex][21 : (21+classSize) ]\n",
    "            for k in range(windowSize):\n",
    "                if predictionIndex != 0:\n",
    "                    dataSet_new[i][j][k] = swapClassLabel(dataSet[i][j+k], classLabel, classSize)\n",
    "                else:\n",
    "                    dataSet_new[i][j][k] = swapClassLabel(dataSet[i][j+k+1], classLabel, classSize)\n",
    "            \n",
    "    return dataSet_new    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeWindowsWithPadding(dataSet, windowSize, numberOfFeatures):\n",
    "    \n",
    "    dataSet = np.reshape(dataSet, (dataSet.shape[0]*dataSet.shape[1], windowSize, numberOfFeatures))\n",
    "    dataSet = dataSet[np.count_nonzero( dataSet, axis=(1,2))>(int(windowSize/2)*23), :, :] \n",
    "    \n",
    "    return dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reshaped_dataset(X_train, X_valid, reductionFunction, numberOfFeatures, classSize, predictionIndex, windowSize):\n",
    "    print(X_train.shape)\n",
    "    X_train = changeQ8Class(X_train, reductionFunction, numberOfFeatures)\n",
    "    print(X_train.shape, \"changed train data to class of size \", classSize)\n",
    "    X_train_window = proteinSequenceToWindowSequence(windowSize,predictionIndex, X_train, classSize)\n",
    "    print(X_train_window.shape, \"changed train data  to window sequence of size \", windowSize)\n",
    "    X_train_window = removeWindowsWithPadding(X_train_window , windowSize, numberOfFeatures)\n",
    "    print(X_train_window.shape, \"filtered windows withouth padding of train data \")\n",
    "\n",
    "    print(X_valid.shape)\n",
    "    X_valid = changeQ8Class(X_valid, reductionFunction, numberOfFeatures)\n",
    "    print(X_valid.shape, \"changed validation data to class size \", classSize)\n",
    "    X_valid_window = proteinSequenceToWindowSequence(windowSize,predictionIndex, X_valid, classSize)\n",
    "    print(X_valid_window.shape, \"changed validation data to window sequence of size \", windowSize)\n",
    "    X_valid_window = removeWindowsWithPadding(X_valid_window , windowSize, numberOfFeatures)\n",
    "    print(X_valid_window.shape, \"filtered windows withouth padding of validation data\")\n",
    "    \n",
    "    return X_train_window, X_valid_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_to_tapped(oneHot, index, windowSize):\n",
    "\n",
    "    oneHot = oneHot.copy()\n",
    "    newOne = 0.5/(windowSize - 1) * index + 0.5\n",
    "    newZero = (1 - newOne) / (np.size(oneHot) - 1)\n",
    "        \n",
    "    oneHot = oneHot.astype(theano.config.floatX)\n",
    "    oneHot[ oneHot == 1] = newOne\n",
    "    oneHot[ oneHot == 0] = newZero\n",
    "\n",
    "    return oneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tapped_one_dataset(dataSet, windowSize, classSize):\n",
    "    \n",
    "    newDataSet = dataSet.copy()\n",
    "    for i in range(np.size(newDataSet, axis=1)): \n",
    "        for j in range(np.size(newDataSet, axis=1)):\n",
    "\n",
    "            oneHot = one_hot_to_tapped( newDataSet[i, j, 0:21], j, windowSize)\n",
    "            features = np.concatenate((oneHot, newDataSet[i, j, 21:]), axis=None)\n",
    "            newDataSet[i][j] = features\n",
    "    \n",
    "    return newDataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(X_train, X_valid, classSize, pssm = False):\n",
    "\n",
    "    if not pssm:\n",
    "        return (X_train[:,:,0:21], X_train[:,:,21 : (21+classSize)], \n",
    "                X_valid[:,:,0:21], X_valid[:,:,21 : (21+classSize)])\n",
    "    else:\n",
    "        return (X_train[:,:,21+classSize:], X_train[:,:,21 : (21+classSize)],\n",
    "                X_valid[:,:,21+classSize:], X_valid[:,:,21 : (21+classSize)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSize = 19\n",
    "predictionIndex = 9\n",
    "classSize = 8  # 2 or 3 \n",
    "numberOfFeatures = 50 #44 \n",
    "\n",
    "amino_acid_residues = 21\n",
    "num_classes = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_window, X_valid_window = get_reshaped_dataset(X_train, X_valid, q8toQ8, numberOfFeatures, classSize, predictionIndex, windowSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('all_dataset_window19Middle.npy', X_train_window) # save\n",
    "#np.save('X_valid_window19Middle.npy', X_valid_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_window = np.load('X_train_window19Middle.npy') # load\n",
    "X_valid_window = np.load('X_valid_window19Middle.npy') # load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1082350, 19, 21) training data\n",
      "(1082350, 19, 8) labels for training data\n",
      "(51152, 19, 21) validation data\n",
      "(51152, 19, 8) labels for training validation\n"
     ]
    }
   ],
   "source": [
    "#a = get_tapped_one_dataset(X_train_window, windowSize, classSize)\n",
    "#b = get_tapped_one_dataset(X_valid_window, windowSize, classSize)\n",
    "\n",
    "x_train_final, y_train_final, x_valid_final, y_valid_final = get_split(X_train_window, X_valid_window, classSize, pssm = False)\n",
    "print(x_train_final.shape, \"training data\")\n",
    "print(y_train_final.shape, \"labels for training data\")\n",
    "print(x_valid_final.shape, \"validation data\")\n",
    "print(y_valid_final.shape, \"labels for training validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper Parameters\n",
      "\n",
      "Learning Rate: 0.0005\n",
      "Drop out: 0.3\n",
      "Batch dim: 64\n",
      "Number of epochs: 1\n",
      "Regularizers: 1e-04\n",
      "\n",
      "Loss: categorical_crossentropy\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 19, 64)            25600     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 19, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 19, 64)            77888     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 19, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 19, 64)            45120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 19, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 19, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 19, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 19, 64)            77888     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 19, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 19, 64)            45120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 19, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 19, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 19, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 19, 64)            77888     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 19, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 19, 64)            45120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 19, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 19, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 19, 64)            256       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 19, 200)           13000     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 19, 8)             1608      \n",
      "=================================================================\n",
      "Total params: 448,848\n",
      "Trainable params: 447,568\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Conv1D, AveragePooling1D, MaxPooling1D, TimeDistributed, LeakyReLU, BatchNormalization, Flatten\n",
    "from tensorflow.keras import optimizers, callbacks\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "LR = 0.0005\n",
    "drop_out = 0.3\n",
    "batch_dim = 64\n",
    "nn_epochs = 1\n",
    "w_reg = regularizers.l2(0.0001)\n",
    "number_filters = 16\n",
    "\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "\n",
    "m = Sequential()\n",
    "\n",
    "#first convolutional neural netwok\n",
    "m.add(Conv1D( 64 , 19,  strides=1, padding='same', activation='relu', use_bias=True, input_shape=(windowSize, 21), kernel_regularizer=w_reg))\n",
    "m.add(BatchNormalization())\n",
    "\n",
    "m.add(Conv1D( 64, 19,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Conv1D( 64, 11,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "m.add(BatchNormalization())\n",
    "\n",
    "\n",
    "m.add(Conv1D( 64, 19,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Conv1D( 64, 11,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "m.add(BatchNormalization())\n",
    "\n",
    "m.add(Conv1D( 64, 19,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Conv1D( 64, 11,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "m.add(BatchNormalization())\n",
    "\n",
    "\n",
    "\n",
    "#4 dense layer\n",
    "m.add(Dense(200, activation='relu', use_bias=True,  kernel_regularizer=w_reg))\n",
    "\n",
    "#5 softmax output layer\n",
    "m.add(Dense(num_classes, activation = 'softmax'))\n",
    "\n",
    "opt = optimizers.Adam(lr=LR)\n",
    "m.compile(optimizer=opt, loss=loss,metrics=['accuracy', 'mae'])\n",
    "\n",
    "print(\"\\nHyper Parameters\\n\")\n",
    "print(\"Learning Rate: \" + str(LR))\n",
    "print(\"Drop out: \" + str(drop_out))\n",
    "print(\"Batch dim: \" + str(batch_dim))\n",
    "print(\"Number of epochs: \" + str(nn_epochs))\n",
    "print(\"Regularizers: \" + str(w_reg.l2))\n",
    "print(\"\\nLoss: \" + loss + \"\\n\")\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate, Conv1D, BatchNormalization\n",
    "from keras.optimizers import Adagrad, Adam\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "LR = 0.0005\n",
    "drop_out = 0.3\n",
    "batch_dim = 64\n",
    "nn_epochs = 1\n",
    "w_reg = regularizers.l2(0.0001)\n",
    "number_filters = 16\n",
    "\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "\n",
    "\n",
    "input_shape = (windowSize, 21)\n",
    "\n",
    "conv1_input = Input(shape=(windowSize, 21), name='InputWindow')\n",
    "\n",
    "conv_1 = Conv1D( 64 , 19,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network1-filter1')(conv1_input)\n",
    "conv_1 = BatchNormalization(name='BN1')(conv_1)\n",
    "conv_2 = Conv1D( 64 , 11,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network1-filter2')(conv1_input)\n",
    "conv_2 = BatchNormalization(name='BN2')(conv_2)\n",
    "conv_3 = Conv1D( 64 , 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network1-filter3')(conv1_input)\n",
    "conv_3 = BatchNormalization(name='BN3')(conv_3)\n",
    "\n",
    "merge_1 = concatenate([conv_1, conv_2, conv_3], name='Network1')\n",
    "input_for_second = concatenate([conv1_input, merge_1], name='Network1-and-input')\n",
    "\n",
    "\n",
    "\n",
    "conv_4 = Conv1D( 64 , 19,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network2-filter1')(input_for_second)\n",
    "conv_4 = BatchNormalization(name='BN4')(conv_4)\n",
    "conv_5 = Conv1D( 64 , 11,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network2-filter2')(input_for_second)\n",
    "conv_5 = BatchNormalization(name='BN5')(conv_5)\n",
    "conv_6 = Conv1D( 64 , 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network2-filter3')(input_for_second)\n",
    "conv_6 = BatchNormalization(name='BN6')(conv_6)\n",
    "\n",
    "merge_2 = concatenate([conv_4, conv_5, conv_6], name='Network2')\n",
    "input_for_third = concatenate([conv1_input, merge_1, merge_2],name='Network1-Network2-and-input')\n",
    "\n",
    "\n",
    "\n",
    "conv_7 = Conv1D( 64 , 19,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network3-filter1')(input_for_third)\n",
    "conv_7 = BatchNormalization(name='BN7')(conv_7)\n",
    "conv_8 = Conv1D( 64 , 11,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network3-filter2')(input_for_third)\n",
    "conv_8 = BatchNormalization(name='BN8')(conv_8)\n",
    "conv_9 = Conv1D( 64 , 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network3-filter3')(input_for_third)\n",
    "conv_9 = BatchNormalization(name='BN9')(conv_9)\n",
    "\n",
    "merge_3 = concatenate([conv_7, conv_8, conv_9],name='Network3')\n",
    "\n",
    "merge_final = concatenate([merge_1, merge_2, merge_3], name='Final')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "first_dense = Dense(200, activation='relu', use_bias=True,  kernel_regularizer=w_reg, name='last')(merge_final)\n",
    "first_dense = BatchNormalization(name='BN10')(first_dense)\n",
    "final_model_output = Dense(num_classes, activation = 'softmax', name='softmax')(first_dense)\n",
    "\n",
    "m = Model(inputs=conv1_input, outputs=final_model_output)\n",
    "\n",
    "opt = Adam(lr=LR)\n",
    "m.compile(optimizer=opt, loss=loss,metrics=['accuracy', 'mae'])\n",
    "\n",
    "print(\"\\nHyper Parameters\\n\")\n",
    "print(\"Learning Rate: \" + str(LR))\n",
    "print(\"Drop out: \" + str(drop_out))\n",
    "print(\"Batch dim: \" + str(batch_dim))\n",
    "print(\"Number of epochs: \" + str(nn_epochs))\n",
    "print(\"Regularizers: \" + str(w_reg.l2))\n",
    "print(\"\\nLoss: \" + loss + \"\\n\")\n",
    "m.summary()\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Users/Ieremie/Anaconda3/pkgs/graphviz-2.38-hfd603c8_2/Library/bin'\n",
    "from keras.utils import plot_model\n",
    "plot_model(m, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1082350 samples, validate on 51152 samples\n",
      " 156480/1082350 [===>..........................] - ETA: 14:10 - loss: 1.3974 - accuracy: 0.5129 - mae: 0.1544"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-2897edbf7061>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_final\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_final\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnn_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_valid_final\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid_final\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "start_time = timer()\n",
    "history = m.fit(x_train_final, y_train_final, epochs=nn_epochs, batch_size=batch_dim, validation_data=(x_valid_final, y_valid_final) ,shuffle=True)\n",
    "\n",
    "end_time = timer()\n",
    "print(\"\\n\\nTime elapsed: \" + \"{0:.2f}\".format((end_time - start_time)) + \" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracyName = 'accuracyMiddlewindowQ2W19Model2.png'\n",
    "lossName = 'lossMiddlewindowQ2W19Model2.png'\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "#plt.savefig(accuracyName)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "#plt.savefig(lossName)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_name = \"19-11-3-doubleFull\"\n",
    "with open(file_name, 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "m.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "(80119, 19, 50)\n"
     ]
    }
   ],
   "source": [
    "X_test = changeQ8Class(X_test, q8toQ8, numberOfFeatures)\n",
    "X_test_window = proteinSequenceToWindowSequence(windowSize,predictionIndex, X_test, classSize)\n",
    "X_test_window = removeWindowsWithPadding(X_test_window , windowSize, numberOfFeatures)\n",
    "print(X_test_window.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = get_tapped_one_dataset(X_test_window, windowSize, classSize)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80119, 19, 21)\n",
      "(80119, 19, 8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_test_final = X_test_window[:,:,(21+classSize):]\n",
    "#x_test_final = X_test_window[:,:,0:21]\n",
    "y_test_final = X_test_window[:,:,21: (21+classSize)]\n",
    "print(x_test_final.shape)\n",
    "print(y_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_final[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = m.predict(x_train_final[0:1])\n",
    "result.reshape(19,8)\n",
    "print(result[0][5].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80119/80119 [==============================] - 36s 446us/sample - loss: 4.2068 - accuracy: 0.3434 - mae: 0.1668\n",
      "Loss: 4.2068291160391045, Accuracy: 0.3433938, MAE: 0.1668269\n",
      "yes boi\n"
     ]
    }
   ],
   "source": [
    "scores = m.evaluate(x_test_final, y_test_final)\n",
    "print(\"Loss: \" + str(scores[0]) + \", Accuracy: \" + str(scores[1]) + \", MAE: \" + str(scores[2]))\n",
    "print(\"yes boi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
