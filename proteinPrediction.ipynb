{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "#import theano\n",
    "import gzip\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Conv1D, AveragePooling1D, MaxPooling1D, TimeDistributed, LeakyReLU, BatchNormalization, Flatten\n",
    "from tensorflow.keras import optimizers, callbacks\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate, Conv1D, BatchNormalization, Flatten\n",
    "from keras.optimizers import Adagrad, Adam\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.utils import plot_model\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSize = 19 #19\n",
    "predictionIndex = 9\n",
    "classSize = 8  # 2 or 3 \n",
    "numberOfFeatures = 50 #50 #44 #45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datset_Casp(caspUpTo):\n",
    "\n",
    "    filename = \"proteinNet-30thinning-all-windows-19-middle-repeating-left-withTranskaya.npy\"\n",
    "    casp = np.memmap(filename, dtype='float64', mode='r+', shape=(4355098 + 1129862, 19, 50))\n",
    "    print(casp.shape)\n",
    "    \n",
    "      \n",
    "    path = \"m-ass.hdf5\"\n",
    "    m = load_model( path )\n",
    "    print(\"model loaded \")\n",
    "\n",
    "    pred = m.predict(casp[1129862:caspUpTo,:,29:], verbose = 1)\n",
    "    print(\"pred made\")\n",
    "    casp[1129862:caspUpTo,0,21:29] = pred\n",
    "    del pred\n",
    "    \n",
    "    f = gzip.GzipFile('X_valid_window19Middle-repeating-left-right0.npy.gz', \"r\")\n",
    "    X_valid_window  = np.load(f)\n",
    "    print(X_valid_window.shape)\n",
    "\n",
    "    print(casp[:,:,29:].shape, \"training data\")\n",
    "    print(casp[:,0,21:29].shape, \"labels for training data\")\n",
    "    print(X_valid_window[:,:,35:56].shape, \"validation data\")\n",
    "    print(X_valid_window[:,0,22:30].shape, \"labels for training validation\")\n",
    "    \n",
    "    return casp[:caspUpTo,:,29:],casp[:caspUpTo,0,21:29],X_valid_window[:,:,35:56],X_valid_window[:,0,22:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_final,y_train_final,x_valid_final,y_valid_final = get_datset_Casp(1129862 * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    \n",
    "    f = gzip.GzipFile('X_train_window19Middle-repeating-left-right0.npy.gz', \"r\")\n",
    "    X_train_window = np.load(f)\n",
    "    \n",
    "    f = gzip.GzipFile('X_valid_window19Middle-repeating-left-right0.npy.gz', \"r\")\n",
    "    X_valid_window  = np.load(f)\n",
    "    \n",
    "    X_train_window = np.concatenate((X_train_window, X_valid_window), axis = 0)\n",
    "    print(X_train_window.shape)\n",
    "    \n",
    "    \n",
    "    X_train_window, X_valid_window = train_test_split(X_train_window, test_size=0.1)\n",
    "    #x_train_final, y_train_final, x_valid_final, y_valid_final = get_split(X_train_window, X_valid_window, classSize, pssm = True)\n",
    "\n",
    "    del X_valid_window\n",
    "    f = gzip.GzipFile('cb513-window19Middle-repeating-left-right0.npy.gz', \"r\")\n",
    "    X_valid_window  = np.load(f)\n",
    "    \n",
    "    x_train_final = X_train_window[:,:,35:56]\n",
    "    y_train_final = X_train_window[:,:,22:30]\n",
    "\n",
    "    x_valid_final = X_valid_window[:,:,35:56]\n",
    "    y_valid_final = X_valid_window[:,:,22:30]\n",
    "\n",
    "    print(x_train_final.shape, \"training data\")\n",
    "    print(y_train_final.shape, \"labels for training data\")\n",
    "    print(x_valid_final.shape, \"validation data\")\n",
    "    print(y_valid_final.shape, \"labels for training validation\")\n",
    "\n",
    "    y_train_final = y_train_final[:,0,:]\n",
    "    print(y_train_final.shape)\n",
    "\n",
    "    y_valid_final = y_valid_final[:,0,:]\n",
    "    print(y_valid_final.shape)\n",
    "    \n",
    "    return x_train_final,y_train_final,x_valid_final,y_valid_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ac5bd782bb13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mwindowSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m21\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mconv1_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindowSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m21\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'InputWindow'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mconv_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv1_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mconv_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'BN1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Anaconda3\\envs\\gpu-cuda10\\lib\\site-packages\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[1;34m(shape, batch_shape, name, dtype, sparse, tensor)\u001b[0m\n\u001b[0;32m    176\u001b[0m                              \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                              \u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m                              input_tensor=tensor)\n\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[1;31m# Return tensor including _keras_shape and _keras_history.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;31m# Note that in this case train_output and test_output are the same pointer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Anaconda3\\envs\\gpu-cuda10\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Anaconda3\\envs\\gpu-cuda10\\lib\\site-packages\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)\u001b[0m\n\u001b[0;32m     85\u001b[0m                                          \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                                          \u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m                                          name=self.name)\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_placeholder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Anaconda3\\envs\\gpu-cuda10\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(shape, ndim, dtype, sparse, name)\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_placeholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_learning_phase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv1D, AveragePooling1D, MaxPooling1D, TimeDistributed, LeakyReLU, BatchNormalization, Flatten\n",
    "from keras import optimizers, callbacks\n",
    "from keras.regularizers import l2\n",
    "# import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "nn_epochs = 15\n",
    "\n",
    "   \n",
    "LR = 0.0009 # maybe after some (10-15) epochs reduce it to 0.0008-0.0007\n",
    "drop_out = 0.4\n",
    "batch_dim = 64\n",
    "\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "input_shape = (windowSize, 21)\n",
    "\n",
    "conv1_input = Input(shape=(windowSize, 21), name='InputWindow')\n",
    "conv_1 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True)(conv1_input)\n",
    "conv_1 = BatchNormalization(name='BN1')(conv_1)\n",
    "conv_2 = Conv1D( 64, 3, strides=1, padding='same', activation='relu', use_bias=True)(conv_1)\n",
    "conv_2 = BatchNormalization(name='BN2')(conv_2)\n",
    "conv_2 = Dropout(drop_out)(conv_2)\n",
    "conv_3 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True)(conv_2)\n",
    "conv_3 = BatchNormalization(name='BN3')(conv_3)\n",
    "conv_3 = Dropout(drop_out)(conv_3)\n",
    "\n",
    "\n",
    "\n",
    "conv_4 = Conv1D( 64, 3, strides=1, padding='same', activation='relu', use_bias=True)(conv_3)\n",
    "conv_4 = BatchNormalization(name='BN4')(conv_4)\n",
    "conv_4 = Dropout(drop_out)(conv_4)\n",
    "conv_5 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True)(conv_4)\n",
    "conv_5 = BatchNormalization(name='BN5')(conv_5)\n",
    "conv_5 = Dropout(drop_out)(conv_5)\n",
    "    #conv_6 = Conv1D( 73, 3, strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_5)\n",
    "    #conv_6 = BatchNormalization(name='BN6')(conv_6)\n",
    "\n",
    "\n",
    "    #conv_7 = Conv1D( 73, 3,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg)(conv_6)\n",
    "    #conv_7 = BatchNormalization(name='BN7')(conv_7)\n",
    "    #conv_8 = Conv1D( 4, 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_7)\n",
    "    #conv_8 = BatchNormalization(name='BN8')(conv_8)\n",
    "    #conv_9 = Conv1D( 4, 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_8)\n",
    "    #conv_9 = BatchNormalization(name='BN9')(conv_9)\n",
    "\n",
    "\n",
    "flatten  = Flatten()(conv_1)\n",
    "first_dense = Dense(16, activation='relu', use_bias=True)(flatten)\n",
    "first_dense = BatchNormalization(name='BN1d')(first_dense)\n",
    "#second_dense = Dense(32, activation='relu', use_bias=True)(first_dense)\n",
    "\n",
    "final_model_output = Dense(classSize, activation = 'softmax', name='softmax')(first_dense)\n",
    "\n",
    "m = Model(inputs=conv1_input, outputs=final_model_output)\n",
    "\n",
    "opt = Adam(lr=LR)\n",
    "m.compile(optimizer=opt, loss=loss,metrics=['accuracy', 'mae'])\n",
    "\n",
    "print(\"\\nHyper Parameters\\n\")\n",
    "print(\"Learning Rate: \" + str(LR))\n",
    "print(\"Drop out: \" + str(drop_out))\n",
    "print(\"Batch dim: \" + str(batch_dim))\n",
    "print(\"Number of epochs: \" + str(nn_epochs))\n",
    "#print(\"Regularizers: \" + str(w_reg.l2))\n",
    "print(\"\\nLoss: \" + loss + \"\\n\")\n",
    "m.summary()\n",
    "\n",
    "#import os\n",
    " #os.environ[\"PATH\"] += os.pathsep + 'C:/Users/Ieremie/Anaconda3/pkgs/graphviz-2.38-hfd603c8_2/Library/bin'\n",
    "\n",
    " #plot_model(m)#, to_file='model.png')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv1D, AveragePooling1D, MaxPooling1D, TimeDistributed, LeakyReLU, BatchNormalization, Flatten\n",
    "from keras import optimizers, callbacks\n",
    "from keras.regularizers import l2\n",
    "# import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_model():\n",
    "\n",
    "\n",
    "    LR = 0.0009 # maybe after some (10-15) epochs reduce it to 0.0008-0.0007\n",
    "    drop_out = 0.6\n",
    "    #nn_epochs = 20\n",
    "    w_reg = regularizers.l2(0) # 0.0003)\n",
    "    number_filters = 16\n",
    "\n",
    "    loss = 'categorical_crossentropy'\n",
    "\n",
    "\n",
    "\n",
    "    input_shape = (windowSize, 21)\n",
    "\n",
    "    conv1_input = Input(shape=(windowSize, 21), name='InputWindow')\n",
    "\n",
    "    conv_1 = Conv1D( 64 , 19,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network1-filter1')(conv1_input)\n",
    "    conv_1 = BatchNormalization(name='BN1')(conv_1)\n",
    "    conv_1 = Dropout(drop_out)(conv_1)\n",
    "    conv_2 = Conv1D( 64 , 11,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network1-filter2')(conv1_input)\n",
    "    conv_2 = BatchNormalization(name='BN2')(conv_2)\n",
    "    conv_2 = Dropout(drop_out)(conv_2)\n",
    "    conv_3 = Conv1D( 64 , 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network1-filter3')(conv1_input)\n",
    "    conv_3 = BatchNormalization(name='BN3')(conv_3)\n",
    "    conv_3 = Dropout(drop_out)(conv_3)\n",
    "\n",
    "    merge_1 = concatenate([conv_1, conv_2, conv_3], name='Network1')\n",
    "    input_for_second = concatenate([conv1_input, merge_1], name='Network1-and-input')\n",
    "\n",
    "\n",
    "\n",
    "    conv_4 = Conv1D( 64 , 19,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network2-filter1')(input_for_second)\n",
    "    conv_4 = BatchNormalization(name='BN4')(conv_4)\n",
    "    conv_4 = Dropout(drop_out)(conv_4)\n",
    "    conv_5 = Conv1D( 64 , 11,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network2-filter2')(input_for_second)\n",
    "    conv_5 = BatchNormalization(name='BN5')(conv_5)\n",
    "    conv_5 = Dropout(drop_out)(conv_5)\n",
    "    conv_6 = Conv1D( 64 , 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network2-filter3')(input_for_second)\n",
    "    conv_6 = BatchNormalization(name='BN6')(conv_6)\n",
    "    conv_6 = Dropout(drop_out)(conv_6)\n",
    "\n",
    "    merge_2 = concatenate([conv_4, conv_5, conv_6], name='Network2')\n",
    "    input_for_third = concatenate([conv1_input, merge_1, merge_2],name='Network1-Network2-and-input')\n",
    "\n",
    "\n",
    "\n",
    "    conv_7 = Conv1D( 64 , 19,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network3-filter1')(input_for_third)\n",
    "    conv_7 = BatchNormalization(name='BN7')(conv_7)\n",
    "    conv_7 = Dropout(drop_out)(conv_7)\n",
    "    conv_8 = Conv1D( 64 , 11,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network3-filter2')(input_for_third)\n",
    "    conv_8 = BatchNormalization(name='BN8')(conv_8)\n",
    "    conv_8 = Dropout(drop_out)(conv_8)\n",
    "    conv_9 = Conv1D( 64 , 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network3-filter3')(input_for_third)\n",
    "    conv_9 = BatchNormalization(name='BN9')(conv_9)\n",
    "    conv_9 = Dropout(drop_out)(conv_9)\n",
    "\n",
    "    merge_3 = concatenate([conv_7, conv_8, conv_9],name='Network3')\n",
    "    input_for_4 = concatenate([conv1_input, merge_1, merge_2, merge_3],name='Network123-and-input')\n",
    "\n",
    "\n",
    "\n",
    "    conv_10 = Conv1D( 64 , 19,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network4-filter1')(input_for_4)\n",
    "    conv_10 = BatchNormalization(name='BN10')(conv_10)\n",
    "    conv_10 = Dropout(drop_out)(conv_10)\n",
    "    conv_11 = Conv1D( 64 , 11,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network4-filter2')(input_for_4)\n",
    "    conv_11 = BatchNormalization(name='BN11')(conv_11)\n",
    "    conv_11 = Dropout(drop_out)(conv_11)\n",
    "    conv_12 = Conv1D( 64 , 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network4-filter3')(input_for_4)\n",
    "    conv_12 = BatchNormalization(name='BN12')(conv_12)\n",
    "    conv_12 = Dropout(drop_out)(conv_12)\n",
    "\n",
    "    merge_4 = concatenate([conv_10, conv_11, conv_12],name='Network4')\n",
    "    input_for_5 = concatenate([conv1_input, merge_1, merge_2, merge_3, merge_4],name='Network1234-and-input')\n",
    "\n",
    "\n",
    "\n",
    "    conv_13 = Conv1D( 64 , 19,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network5-filter1')(input_for_5)\n",
    "    conv_13 = BatchNormalization(name='BN13')(conv_13)\n",
    "    conv_13 = Dropout(drop_out)(conv_13)\n",
    "    conv_14 = Conv1D( 64 , 11,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network5-filter2')(input_for_5)\n",
    "    conv_14 = BatchNormalization(name='BN14')(conv_14)\n",
    "    conv_14 = Dropout(drop_out)(conv_14)\n",
    "    conv_15 = Conv1D( 64 , 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network5-filter3')(input_for_5)\n",
    "    conv_15 = BatchNormalization(name='BN15')(conv_15)\n",
    "    conv_15 = Dropout(drop_out)(conv_15)\n",
    "\n",
    "    merge_5 = concatenate([conv_13, conv_14, conv_15],name='Network5')\n",
    "\n",
    "\n",
    "\n",
    "    merge_final = concatenate([merge_1, merge_2, merge_3,merge_4, merge_5], name='Final')\n",
    "\n",
    "\n",
    "\n",
    "    flatten  = Flatten()(merge_final)\n",
    "    first_dense = Dense(128, activation='relu', use_bias=True,  kernel_regularizer=w_reg, name='last')(flatten)\n",
    "    first_dense = BatchNormalization(name='BN16')(first_dense)\n",
    "\n",
    "    #second_dense = Dense(32, activation='relu', use_bias=True,  kernel_regularizer=w_reg, name='last')(flatten)\n",
    "    #second_dense = BatchNormalization(name='BN16')(second_dense)\n",
    "\n",
    "    final_model_output = Dense(classSize, activation = 'softmax', name='softmax')(first_dense)\n",
    "\n",
    "    m = Model(inputs=conv1_input, outputs=final_model_output)\n",
    "\n",
    "    opt = Adam(lr=LR)\n",
    "    m.compile(optimizer=opt, loss=loss,metrics=['accuracy', 'mae'])\n",
    "\n",
    "    print(\"\\nHyper Parameters\\n\")\n",
    "    print(\"Learning Rate: \" + str(LR))\n",
    "    print(\"Drop out: \" + str(drop_out))\n",
    "    print(\"Batch dim: \" + str(batch_dim))\n",
    "    print(\"Number of epochs: \" + str(nn_epochs))\n",
    "    print(\"Regularizers: \" + str(w_reg.l2))\n",
    "    print(\"\\nLoss: \" + loss + \"\\n\")\n",
    "    m.summary()\n",
    "\n",
    "    #import os\n",
    "    #os.environ[\"PATH\"] += os.pathsep + 'C:/Users/Ieremie/Anaconda3/pkgs/graphviz-2.38-hfd603c8_2/Library/bin'\n",
    "    #from keras.utils import plot_model\n",
    "    #plot_model(m)#, to_file='model.png')\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from time import time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "nr_models_to_train = 10\n",
    "nn_epochs = 45\n",
    "batch_dim = 64\n",
    "\n",
    "for i in range(0,nr_models_to_train):\n",
    "    \n",
    "    \n",
    "    x_train_final,y_train_final,x_valid_final,y_valid_final = get_dataset()\n",
    "    \n",
    "    #callbacks\n",
    "    filepath=\"//filestore.soton.ac.uk/users/cid1u17/model-assembly-shuffle-cb513-45epochs-\" + str(i) + \".hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max', period=1)\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    m = get_model()\n",
    "\n",
    "    #training \n",
    "    start_time = timer()\n",
    "    history = m.fit(x_train_final, y_train_final, epochs=nn_epochs, batch_size=batch_dim,\n",
    "                validation_data=(x_valid_final, y_valid_final) ,shuffle=True,  callbacks=callbacks_list)\n",
    "\n",
    "    end_time = timer()\n",
    "    print(\"\\n\\nTime elapsed: \" + \"{0:.2f}\".format((end_time - start_time)) + \" s\")\n",
    "    \n",
    "    \n",
    "    #saving results\n",
    "    with open('//filestore.soton.ac.uk/users/cid1u17/model-assembly-shuffle-cb513-hist-45epochs-' + str(i), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "accuracyName = 'accuracyMiddlewindowQ2W19Model2.png'\n",
    "lossName = 'lossMiddlewindowQ2W19Model2.png'\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "#plt.savefig(accuracyName)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "#plt.savefig(lossName)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.GzipFile('cb513_window19Q8.npy.gz', \"r\")\n",
    "X_test_window = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_final = X_test_window[:,:,(21+classSize):]\n",
    "#x_test_final = X_test_window[:,:,0:21]\n",
    "y_test_final = X_test_window[:,:,21: (21+classSize)]\n",
    "print(x_test_final.shape)\n",
    "print(y_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_final = np.reshape(y_test_final, (80119,19*8))\n",
    "print(y_test_final.shape)\n",
    "y_test_final = y_test_final[:,0:8]\n",
    "print(y_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test_final, y_test_final)\n",
    "print(\"Loss: \" + str(scores[0]) + \", Accuracy: \" + str(scores[1]) + \", MAE: \" + str(scores[2]))\n",
    "print(\"yes boi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 3\n",
    "m.save('model_' + str(fold) + '.h5')  # creates a HDF5 file \n",
    "with open('model_scores_' + str(fold), 'wb') as file_pi:\n",
    "    pickle.dump(scores, file_pi)\n",
    "with open('model_history_' + str(fold), 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
