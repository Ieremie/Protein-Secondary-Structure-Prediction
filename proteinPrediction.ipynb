{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'secondary_proteins_prediction/data/cullpdb+profile_6133_filtered.npy.gz'\n",
    "TEST_PATH =  'secondary_proteins_prediction/data/cb513+profile_split1.npy.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gz(path):  # load a .npy.gz file\n",
    "    if path.endswith(\".gz\"):\n",
    "        f = open(path, 'rb')\n",
    "        return np.load(f)\n",
    "    else:\n",
    "        return np.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train(path=TRAIN_PATH):\n",
    "    if not os.path.isfile(path):\n",
    "        print(\"Train path is not downloaded ...\")\n",
    "        subprocess.call(\"./download_train.sh\", shell=True)\n",
    "    else:\n",
    "        print(\"Train path is downloaded ...\")\n",
    "    print(\"Loading train data ...\")\n",
    "    X_in = load_gz(path)\n",
    "    X = np.reshape(X_in, (5534, 700, 57))\n",
    "    del X_in\n",
    "    X = X[:, :, :].astype(theano.config.floatX)\n",
    "  \n",
    "    seq_names = np.arange(0, np.size(X, 0))\n",
    "\n",
    "    X_train = X[seq_names[0:5278]]\n",
    "    X_valid = X[seq_names[5278:5534]]\n",
    "    \n",
    "    return X_train, X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid= get_train(TRAIN_PATH)\n",
    "print(\"Done loading train\")\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test(path=TEST_PATH):\n",
    "    if not os.path.isfile(path):\n",
    "        subprocess.call(\"./download_test.sh\", shell=True)\n",
    "    print(\"Loading test data ...\")\n",
    "    X_test_in = load_gz(path)\n",
    "    X_test = np.reshape(X_test_in, (514, 700, 57))\n",
    "    del X_test_in\n",
    "    X_test = X_test[:, :, :].astype(theano.config.floatX)\n",
    "\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = get_test(TEST_PATH)\n",
    "print(\"Done loading test\")\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do: Make 1 hot encoded class - from Q8 to Q3\n",
    "       #Reshape the dataset so it has 700*windowSize seq that map to a class\n",
    "       #Look inside lasagne to see how we disregard the padding\n",
    "##\n",
    "\n",
    "def q8ClassToQ3(q8Labels):\n",
    "    \n",
    "    q3 = np.zeros(3)\n",
    "    index = np.argmax(q8Labels)\n",
    "    \n",
    "    #Helix\n",
    "    if index == 5 or index == 3 or index == 4 : # H ,G, I\n",
    "        q3[0] = 1   \n",
    "    #beta    \n",
    "    if index == 1 or index == 2: # B, E\n",
    "        q3[1] = 1    \n",
    "    #coil    \n",
    "    if index == 7 or index == 6 or index == 0 : # T, S, L\n",
    "        q3[2] = 1\n",
    "    return q3\n",
    "\n",
    "def q8ClassToQ2(q8Labels):\n",
    "    \n",
    "    q2 = np.zeros(2)\n",
    "    index = np.argmax(q8Labels)\n",
    "    #Helix\n",
    "    if index == 5: # or index == 3 or index == 4 : # H ,G, I\n",
    "        q2[0] = 1\n",
    "    else:\n",
    "        q2[1] = 1  \n",
    "    return q2\n",
    "\n",
    "def changeQ8Class(dataSet, reductionFunction, numberOfFeatures):\n",
    "\n",
    "    num_seqs = np.size(dataSet, 0)\n",
    "    seqlen = np.size(dataSet, 1)\n",
    "    labels_new = np.zeros((num_seqs, seqlen, numberOfFeatures))\n",
    "\n",
    "    for i in range(np.size(dataSet, axis=0)):\n",
    "        for j in range(np.size(dataSet, axis=1)):\n",
    "            oneHot = reductionFunction(dataSet[i, j, 22:30])\n",
    "            features = np.concatenate((dataSet[i, j, 0:21], oneHot), axis=None)\n",
    "            features = np.concatenate((features, dataSet[i, j, 35:56]), axis=None)\n",
    "            labels_new[i][j] = features\n",
    "    return labels_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swapClassLabel(features, classLabel, classSize):\n",
    "\n",
    "    res = np.concatenate((features[0:21], classLabel), axis=None)\n",
    "    res = np.concatenate((res, features[ (21+classSize) :]), axis=None)\n",
    "  \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#num_classes should be q8, q3 or maybe q2?\n",
    "def proteinSequenceToWindowSequence(windowSize, predictionIndex , dataSet, classSize):\n",
    "   \n",
    "    num_seqs = np.size(dataSet, 0)\n",
    "    seqlen = np.size(dataSet, 1)\n",
    "    features = np.size(dataSet, 2)\n",
    "    dataSet_new = np.zeros((num_seqs, seqlen - windowSize + 1, windowSize, features))\n",
    "    \n",
    "    for i in range(np.size(dataSet, axis=0)):\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        for j in range(np.size(dataSet, axis=1) - windowSize):\n",
    "            classLabel = dataSet[i][j + predictionIndex][21 : (21+classSize) ]\n",
    "            for k in range(windowSize):\n",
    "                if predictionIndex != 0:\n",
    "                    dataSet_new[i][j][k] = swapClassLabel(dataSet[i][j+k], classLabel, classSize)\n",
    "                else:\n",
    "                    dataSet_new[i][j][k] = swapClassLabel(dataSet[i][j+k+1], classLabel, classSize)\n",
    "            \n",
    "    return dataSet_new    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeWindowsWithPadding(dataSet, windowSize, numberOfFeatures):\n",
    "    \n",
    "    dataSet = np.reshape(dataSet, (dataSet.shape[0]*dataSet.shape[1], windowSize, numberOfFeatures))\n",
    "    dataSet = dataSet[np.count_nonzero( dataSet, axis=(1,2))>(int(windowSize/2)*numberOfFeatures), :, :] \n",
    "    \n",
    "    return dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reshaped_dataset(X_train, X_valid, reductionFunction, numberOfFeatures):\n",
    "    print(X_train.shape)\n",
    "    X_train = changeQ8Class(X_train, reductionFunction, numberOfFeatures)\n",
    "    print(X_train.shape, \"changed train data to class of size \", classSize)\n",
    "    X_train_window = proteinSequenceToWindowSequence(windowSize,predictionIndex, X_train, classSize)\n",
    "    print(X_train_window.shape, \"changed train data  to window sequence of size \", windowSize)\n",
    "    X_train_window = removeWindowsWithPadding(X_train_window , windowSize, numberOfFeatures)\n",
    "    print(X_train_window.shape, \"filtered windows withouth padding of train data \")\n",
    "\n",
    "    print(X_valid.shape)\n",
    "    X_valid = changeQ8Class(X_valid, reductionFunction, numberOfFeatures)\n",
    "    print(X_valid.shape, \"changed validation data to class size \", classSize)\n",
    "    X_valid_window = proteinSequenceToWindowSequence(windowSize,predictionIndex, X_valid, classSize)\n",
    "    print(X_valid_window.shape, \"changed validation data to window sequence of size \", windowSize)\n",
    "    X_valid_window = removeWindowsWithPadding(X_valid_window , windowSize, numberOfFeatures)\n",
    "    print(X_valid_window.shape, \"filtered windows withouth padding of validation data\")\n",
    "    \n",
    "    return X_train_window, X_valid_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(X_train, X_valid, classSize, pssm = False):\n",
    "\n",
    "    if not pssm:\n",
    "        return (X_train[:,:,0:21], X_train[:,:,21 : (21+classSize)], \n",
    "                X_valid[:,:,0:21], X_valid[:,:,21 : (21+classSize)])\n",
    "    else:\n",
    "        return (X_train[:,:,21+classSize:], X_train[:,:,21 : (21+classSize)],\n",
    "                X_valid[:,:,21+classSize:], X_valid[:,:,21 : (21+classSize)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSize = 9\n",
    "predictionIndex = 0\n",
    "classSize = 2  # 2 or 3 \n",
    "numberOfFeatures = 44\n",
    "\n",
    "amino_acid_residues = 21\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_window, X_valid_window = get_reshaped_dataset(X_train, X_valid, q8ClassToQ2, numberOfFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_final, y_train_final, x_valid_final, y_valid_final = get_split(X_train_window, X_valid_window, classSize, pssm = True)\n",
    "print(x_train_final.shape, \"training data\")\n",
    "print(y_train_final.shape, \"labels for training data\")\n",
    "print(x_valid_final.shape, \"validation data\")\n",
    "print(y_valid_final.shape, \"labels for training validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_final[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Conv1D, AveragePooling1D, MaxPooling1D, TimeDistributed, LeakyReLU, BatchNormalization, Flatten\n",
    "from tensorflow.keras import optimizers, callbacks\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "LR = 0.0005\n",
    "drop_out = 0.3\n",
    "batch_dim = 64\n",
    "nn_epochs = 10\n",
    "\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "\n",
    "m = Sequential()\n",
    "m.add(Conv1D(128, 11, padding='same'\n",
    "             , activation='relu', input_shape=(windowSize, amino_acid_residues)))\n",
    "m.add(Dropout(drop_out))\n",
    "m.add(Conv1D(64, 11, padding='same', activation='relu'))\n",
    "m.add(Dropout(drop_out))\n",
    "m.add(Conv1D(num_classes, 11, padding='same', activation='softmax'))\n",
    "opt = optimizers.Adam(lr=LR)\n",
    "m.compile(optimizer=opt, loss=loss,metrics=['accuracy', 'mae'])\n",
    "\n",
    "print(\"\\nHyper Parameters\\n\")\n",
    "print(\"Learning Rate: \" + str(LR))\n",
    "print(\"Drop out: \" + str(drop_out))\n",
    "print(\"Batch dim: \" + str(batch_dim))\n",
    "print(\"Number of epochs: \" + str(nn_epochs))\n",
    "print(\"\\nLoss: \" + loss + \"\\n\")\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Conv1D, AveragePooling1D, MaxPooling1D, TimeDistributed, LeakyReLU, BatchNormalization, Flatten\n",
    "from tensorflow.keras import optimizers, callbacks\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "LR = 0.0005\n",
    "drop_out = 0.3\n",
    "batch_dim = 64\n",
    "nn_epochs = 10\n",
    "w_reg = regularizers.l2(0.0001)\n",
    "number_filters = 16\n",
    "\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "\n",
    "m = Sequential()\n",
    "\n",
    "#first convolutional neural netwok\n",
    "m.add(Conv1D( 128, 3,  strides=1, padding='same', activation='relu', use_bias=True, input_shape=(windowSize, amino_acid_residues), kernel_regularizer=w_reg))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Conv1D( 128, 5,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Conv1D( 64, 7,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "m.add(BatchNormalization())\n",
    "\n",
    "\n",
    "#2 \n",
    "m.add(Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True ,kernel_regularizer=w_reg))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Conv1D( 32, 5,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Conv1D( 16, 7,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "m.add(BatchNormalization())\n",
    "\n",
    "#3 \n",
    "m.add(Conv1D( 16, 3,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Conv1D( 16, 5,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Conv1D( 8, 7,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "m.add(BatchNormalization())\n",
    "\n",
    "\n",
    "#4 dense layer\n",
    "m.add(Dense(200, activation='relu', use_bias=True,  kernel_regularizer=w_reg))\n",
    "\n",
    "#5 softmax output layer\n",
    "m.add(Dense(num_classes, activation = 'softmax'))\n",
    "\n",
    "opt = optimizers.Adam(lr=LR)\n",
    "m.compile(optimizer=opt, loss=loss,metrics=['accuracy', 'mae'])\n",
    "\n",
    "print(\"\\nHyper Parameters\\n\")\n",
    "print(\"Learning Rate: \" + str(LR))\n",
    "print(\"Drop out: \" + str(drop_out))\n",
    "print(\"Batch dim: \" + str(batch_dim))\n",
    "print(\"Number of epochs: \" + str(nn_epochs))\n",
    "print(\"\\nLoss: \" + loss + \"\\n\")\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0009 # maybe after some (10-15) epochs reduce it to 0.0008-0.0007\n",
    "drop_out = 0.38\n",
    "batch_dim = 64\n",
    "\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "m = Sequential()\n",
    "m.add(Conv1D(128, 5, padding='same', activation='relu', input_shape=(windowSize, amino_acid_residues)))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Dropout(drop_out))\n",
    "m.add(Conv1D(128, 3, padding='same', activation='relu'))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Dropout(drop_out))\n",
    "m.add(Conv1D(64, 3, padding='same', activation='relu'))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Dropout(drop_out))\n",
    "#m.add(Flatten())\n",
    "m.add(Dense(128, activation='relu'))\n",
    "m.add(Dense(32, activation='relu'))\n",
    "m.add(Dense(num_classes, activation = 'softmax'))\n",
    "opt = optimizers.Adam(lr=LR)\n",
    "m.compile(optimizer=opt,\n",
    "          loss=loss,\n",
    "          metrics=['accuracy', 'mae'])\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "start_time = timer()\n",
    "history = m.fit(x_train_final, y_train_final, epochs=nn_epochs, batch_size=batch_dim, validation_data=(x_valid_final, y_valid_final) ,shuffle=True)\n",
    "\n",
    "end_time = timer()\n",
    "print(\"\\n\\nTime elapsed: \" + \"{0:.2f}\".format((end_time - start_time)) + \" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracyName = 'accuracyLeftwindowQ2W9pssm.png'\n",
    "lossName = 'lossLeftwindowQ2W9pssm.png'\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.savefig(accuracyName)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.savefig(lossName)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "pickleName = \"Q2Window9Leftpssm.pickle\"\n",
    "pkl_filename = \"Q2Window19Leftpssm.pkl\"\n",
    "\n",
    "pickle_out = open(pickleName,\"wb\")\n",
    "pickle.dump(history, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(m, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save to file in the current working directory\n",
    "pkl_filename = \"Q2Window19MLeft.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(m, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = changeQ8Class(X_test, q8ClassToQ2, numberOfFeatures)\n",
    "X_test_window = proteinSequenceToWindowSequence(windowSize,predictionIndex, X_test, classSize)\n",
    "X_test_window = removeWindowsWithPadding(X_test_window , windowSize, numberOfFeatures)\n",
    "print(X_train_window.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_test_final = X_test_window[:,:,(21+classSize):]\n",
    "#x_test_final = X_test_window[:,:,0:21]\n",
    "y_test_final = X_test_window[:,:,21: (21+classSize)]\n",
    "print(x_test_final.shape)\n",
    "print(y_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test_final[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = m.evaluate(x_test_final, y_test_final)\n",
    "print(\"Loss: \" + str(scores[0]) + \", Accuracy: \" + str(scores[1]) + \", MAE: \" + str(scores[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
