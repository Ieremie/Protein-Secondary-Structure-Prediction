{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "#import theano\n",
    "import gzip\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Conv1D, AveragePooling1D, MaxPooling1D, TimeDistributed, LeakyReLU, BatchNormalization, Flatten\n",
    "from tensorflow.keras import optimizers, callbacks\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate, Conv1D, BatchNormalization, Flatten\n",
    "from keras.optimizers import Adagrad, Adam\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.utils import plot_model\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(X_train, X_valid, classSize, pssm = False):\n",
    "\n",
    "    if not pssm:\n",
    "        return (X_train[:,:,0:21], X_train[:,:,21 : (21+classSize)], \n",
    "                X_valid[:,:,0:21], X_valid[:,:,21 : (21+classSize)])\n",
    "    else:\n",
    "        return (X_train[:,:,21+classSize:], X_train[:,:,21 : (21+classSize)],\n",
    "                X_valid[:,:,21+classSize:], X_valid[:,:,21 : (21+classSize)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSize = 19 #19\n",
    "predictionIndex = 9\n",
    "classSize = 8  # 2 or 3 \n",
    "numberOfFeatures = 50 #50 #44 #45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datset_Casp(caspUpTo):\n",
    "\n",
    "    filename = \"proteinNet-30thinning-all-windows-19-middle-repeating-left-withTranskaya.npy\"\n",
    "    casp = np.memmap(filename, dtype='float64', mode='r+', shape=(4355098 + 1129862, 19, 50))\n",
    "    print(casp.shape)\n",
    "    \n",
    "      \n",
    "    path = \"m-ass.hdf5\"\n",
    "    m = load_model( path )\n",
    "    print(\"model loaded \")\n",
    "\n",
    "    pred = m.predict(casp[1129862:caspUpTo,:,29:], verbose = 1)\n",
    "    print(\"pred made\")\n",
    "    casp[1129862:caspUpTo,0,21:29] = pred\n",
    "    del pred\n",
    "    \n",
    "    f = gzip.GzipFile('X_valid_window19Middle-repeating-left-right0.npy.gz', \"r\")\n",
    "    X_valid_window  = np.load(f)\n",
    "    print(X_valid_window.shape)\n",
    "\n",
    "    print(casp[:,:,29:].shape, \"training data\")\n",
    "    print(casp[:,0,21:29].shape, \"labels for training data\")\n",
    "    print(X_valid_window[:,:,35:56].shape, \"validation data\")\n",
    "    print(X_valid_window[:,0,22:30].shape, \"labels for training validation\")\n",
    "    \n",
    "    return casp[:,:,29:],casp[:,0,21:29],X_valid_window[:,:,35:56],X_valid_window[:,0,22:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5484960, 19, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0422 15:22:59.225150 20436 nn_ops.py:4283] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0422 15:22:59.240021 20436 nn_ops.py:4283] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0422 15:22:59.253825 20436 nn_ops.py:4283] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0422 15:22:59.410728 20436 nn_ops.py:4283] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0422 15:22:59.432362 20436 nn_ops.py:4283] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded \n"
     ]
    }
   ],
   "source": [
    "x_train_final,y_train_final,x_valid_final,y_valid_final = get_datset_Casp(1129862 * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_casp(caspUpTo):\n",
    "    \n",
    "    f = gzip.GzipFile('X_train_window19Middle-repeating-left-right0.npy.gz', \"r\")\n",
    "    X_train_window = np.load(f)\n",
    "    print(X_train_window.shape, \" X_train_window.shape\")\n",
    "    \n",
    "    f = gzip.GzipFile('proteinNet-30thinning-all-windows-19-middle-repeating-left.npy.gz', \"r\")\n",
    "    casp  = np.load(f)\n",
    "    print(casp.shape, \" casp.shape\")\n",
    "    \n",
    "    \n",
    "    path = \"m-ass.hdf5\"\n",
    "    m = load_model( path )\n",
    "    \n",
    "    casp = casp[:caspUpTo]\n",
    "    casp[:,0,21:29] = m.predict(casp[:,:,29:], verbose = 1)\n",
    "    \n",
    "    X_train_window = np.concatenate((X_train_window[:,:,35:56], casp[:,:,29:]), axis = 0) \n",
    "    casp = np.concatenate((X_train_window[:,:,22:30], casp[:,:,21:29]), axis = 0)\n",
    "    \n",
    "    f = gzip.GzipFile('X_valid_window19Middle-repeating-left-right0.npy.gz', \"r\")\n",
    "    X_valid_window  = np.load(f)\n",
    "    print(X_valid_window.shape, \" X_valid_window.shape\")\n",
    "    \n",
    "    x_valid_final = X_valid_window[:,:,35:56]\n",
    "    y_valid_final = X_valid_window[:,:,22:30]\n",
    "    \n",
    "    del X_valid_window\n",
    "\n",
    "    print(x_train_final.shape, \"training data\")\n",
    "    print(y_train_final.shape, \"labels for training data\")\n",
    "    print(x_valid_final.shape, \"validation data\")\n",
    "    print(y_valid_final.shape, \"labels for training validation\")\n",
    "\n",
    "    y_train_final = y_train_final[:,0,:]\n",
    "    print(y_train_final.shape)\n",
    "\n",
    "    y_valid_final = y_valid_final[:,0,:]\n",
    "    print(y_valid_final.shape)\n",
    "    \n",
    "    return X_train_window,casp,x_valid_final,y_valid_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_final,y_train_final,x_valid_final,y_valid_final = get_dataset_casp(1129862)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    \n",
    "    f = gzip.GzipFile('X_train_window19Middle-repeating-left-right0.npy.gz', \"r\")\n",
    "    X_train_window = np.load(f)\n",
    "    \n",
    "    f = gzip.GzipFile('X_valid_window19Middle-repeating-left-right0.npy.gz', \"r\")\n",
    "    X_valid_window  = np.load(f)\n",
    "    \n",
    "    X_train_window = np.concatenate((X_train_window, X_valid_window), axis = 0)\n",
    "    print(X_train_window.shape)\n",
    "    \n",
    "    \n",
    "    X_train_window, X_valid_window = train_test_split(X_train_window, test_size=0.1)\n",
    "    #x_train_final, y_train_final, x_valid_final, y_valid_final = get_split(X_train_window, X_valid_window, classSize, pssm = True)\n",
    "\n",
    "    x_train_final = X_train_window[:,:,35:56]\n",
    "    y_train_final = X_train_window[:,:,22:30]\n",
    "\n",
    "    x_valid_final = X_valid_window[:,:,35:56]\n",
    "    y_valid_final = X_valid_window[:,:,22:30]\n",
    "\n",
    "    print(x_train_final.shape, \"training data\")\n",
    "    print(y_train_final.shape, \"labels for training data\")\n",
    "    print(x_valid_final.shape, \"validation data\")\n",
    "    print(y_valid_final.shape, \"labels for training validation\")\n",
    "\n",
    "    y_train_final = y_train_final[:,0,:]\n",
    "    print(y_train_final.shape)\n",
    "\n",
    "    y_valid_final = y_valid_final[:,0,:]\n",
    "    print(y_valid_final.shape)\n",
    "    \n",
    "    return x_train_final,y_train_final,x_valid_final,y_valid_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv1D, AveragePooling1D, MaxPooling1D, TimeDistributed, LeakyReLU, BatchNormalization, Flatten\n",
    "from keras import optimizers, callbacks\n",
    "from keras.regularizers import l2\n",
    "# import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "nn_epochs = 15\n",
    "\n",
    "   \n",
    "LR = 0.0009 # maybe after some (10-15) epochs reduce it to 0.0008-0.0007\n",
    "drop_out = 0.4\n",
    "batch_dim = 64\n",
    "\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "input_shape = (windowSize, 21)\n",
    "\n",
    "conv1_input = Input(shape=(windowSize, 21), name='InputWindow')\n",
    "conv_1 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True)(conv1_input)\n",
    "conv_1 = BatchNormalization(name='BN1')(conv_1)\n",
    "conv_2 = Conv1D( 64, 3, strides=1, padding='same', activation='relu', use_bias=True)(conv_1)\n",
    "conv_2 = BatchNormalization(name='BN2')(conv_2)\n",
    "conv_2 = Dropout(drop_out)(conv_2)\n",
    "conv_3 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True)(conv_2)\n",
    "conv_3 = BatchNormalization(name='BN3')(conv_3)\n",
    "conv_3 = Dropout(drop_out)(conv_3)\n",
    "\n",
    "\n",
    "\n",
    "conv_4 = Conv1D( 64, 3, strides=1, padding='same', activation='relu', use_bias=True)(conv_3)\n",
    "conv_4 = BatchNormalization(name='BN4')(conv_4)\n",
    "conv_4 = Dropout(drop_out)(conv_4)\n",
    "conv_5 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True)(conv_4)\n",
    "conv_5 = BatchNormalization(name='BN5')(conv_5)\n",
    "conv_5 = Dropout(drop_out)(conv_5)\n",
    "    #conv_6 = Conv1D( 73, 3, strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_5)\n",
    "    #conv_6 = BatchNormalization(name='BN6')(conv_6)\n",
    "\n",
    "\n",
    "    #conv_7 = Conv1D( 73, 3,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg)(conv_6)\n",
    "    #conv_7 = BatchNormalization(name='BN7')(conv_7)\n",
    "    #conv_8 = Conv1D( 4, 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_7)\n",
    "    #conv_8 = BatchNormalization(name='BN8')(conv_8)\n",
    "    #conv_9 = Conv1D( 4, 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_8)\n",
    "    #conv_9 = BatchNormalization(name='BN9')(conv_9)\n",
    "\n",
    "\n",
    "flatten  = Flatten()(conv_1)\n",
    "first_dense = Dense(16, activation='relu', use_bias=True)(flatten)\n",
    "first_dense = BatchNormalization(name='BN1d')(first_dense)\n",
    "#second_dense = Dense(32, activation='relu', use_bias=True)(first_dense)\n",
    "\n",
    "final_model_output = Dense(classSize, activation = 'softmax', name='softmax')(first_dense)\n",
    "\n",
    "m = Model(inputs=conv1_input, outputs=final_model_output)\n",
    "\n",
    "opt = Adam(lr=LR)\n",
    "m.compile(optimizer=opt, loss=loss,metrics=['accuracy', 'mae'])\n",
    "\n",
    "print(\"\\nHyper Parameters\\n\")\n",
    "print(\"Learning Rate: \" + str(LR))\n",
    "print(\"Drop out: \" + str(drop_out))\n",
    "print(\"Batch dim: \" + str(batch_dim))\n",
    "print(\"Number of epochs: \" + str(nn_epochs))\n",
    "#print(\"Regularizers: \" + str(w_reg.l2))\n",
    "print(\"\\nLoss: \" + loss + \"\\n\")\n",
    "m.summary()\n",
    "\n",
    "#import os\n",
    " #os.environ[\"PATH\"] += os.pathsep + 'C:/Users/Ieremie/Anaconda3/pkgs/graphviz-2.38-hfd603c8_2/Library/bin'\n",
    "\n",
    " #plot_model(m)#, to_file='model.png')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv1D, AveragePooling1D, MaxPooling1D, TimeDistributed, LeakyReLU, BatchNormalization, Flatten\n",
    "from keras import optimizers, callbacks\n",
    "from keras.regularizers import l2\n",
    "# import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_model():\n",
    "\n",
    "\n",
    "    LR = 0.0009 # maybe after some (10-15) epochs reduce it to 0.0008-0.0007\n",
    "    drop_out = 0.6\n",
    "    #nn_epochs = 20\n",
    "    w_reg = regularizers.l2(0) # 0.0003)\n",
    "    number_filters = 16\n",
    "\n",
    "    loss = 'categorical_crossentropy'\n",
    "\n",
    "\n",
    "\n",
    "    input_shape = (windowSize, 21)\n",
    "\n",
    "    conv1_input = Input(shape=(windowSize, 21), name='InputWindow')\n",
    "\n",
    "    conv_1 = Conv1D( 64 , 19,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network1-filter1')(conv1_input)\n",
    "    conv_1 = BatchNormalization(name='BN1')(conv_1)\n",
    "    conv_1 = Dropout(drop_out)(conv_1)\n",
    "    conv_2 = Conv1D( 64 , 11,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network1-filter2')(conv1_input)\n",
    "    conv_2 = BatchNormalization(name='BN2')(conv_2)\n",
    "    conv_2 = Dropout(drop_out)(conv_2)\n",
    "    conv_3 = Conv1D( 64 , 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network1-filter3')(conv1_input)\n",
    "    conv_3 = BatchNormalization(name='BN3')(conv_3)\n",
    "    conv_3 = Dropout(drop_out)(conv_3)\n",
    "\n",
    "    merge_1 = concatenate([conv_1, conv_2, conv_3], name='Network1')\n",
    "    input_for_second = concatenate([conv1_input, merge_1], name='Network1-and-input')\n",
    "\n",
    "\n",
    "\n",
    "    conv_4 = Conv1D( 64 , 19,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network2-filter1')(input_for_second)\n",
    "    conv_4 = BatchNormalization(name='BN4')(conv_4)\n",
    "    conv_4 = Dropout(drop_out)(conv_4)\n",
    "    conv_5 = Conv1D( 64 , 11,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network2-filter2')(input_for_second)\n",
    "    conv_5 = BatchNormalization(name='BN5')(conv_5)\n",
    "    conv_5 = Dropout(drop_out)(conv_5)\n",
    "    conv_6 = Conv1D( 64 , 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network2-filter3')(input_for_second)\n",
    "    conv_6 = BatchNormalization(name='BN6')(conv_6)\n",
    "    conv_6 = Dropout(drop_out)(conv_6)\n",
    "\n",
    "    merge_2 = concatenate([conv_4, conv_5, conv_6], name='Network2')\n",
    "    input_for_third = concatenate([conv1_input, merge_1, merge_2],name='Network1-Network2-and-input')\n",
    "\n",
    "\n",
    "\n",
    "    conv_7 = Conv1D( 64 , 19,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network3-filter1')(input_for_third)\n",
    "    conv_7 = BatchNormalization(name='BN7')(conv_7)\n",
    "    conv_7 = Dropout(drop_out)(conv_7)\n",
    "    conv_8 = Conv1D( 64 , 11,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network3-filter2')(input_for_third)\n",
    "    conv_8 = BatchNormalization(name='BN8')(conv_8)\n",
    "    conv_8 = Dropout(drop_out)(conv_8)\n",
    "    conv_9 = Conv1D( 64 , 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network3-filter3')(input_for_third)\n",
    "    conv_9 = BatchNormalization(name='BN9')(conv_9)\n",
    "    conv_9 = Dropout(drop_out)(conv_9)\n",
    "\n",
    "    merge_3 = concatenate([conv_7, conv_8, conv_9],name='Network3')\n",
    "    input_for_4 = concatenate([conv1_input, merge_1, merge_2, merge_3],name='Network123-and-input')\n",
    "\n",
    "\n",
    "\n",
    "    conv_10 = Conv1D( 64 , 19,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network4-filter1')(input_for_4)\n",
    "    conv_10 = BatchNormalization(name='BN10')(conv_10)\n",
    "    conv_10 = Dropout(drop_out)(conv_10)\n",
    "    conv_11 = Conv1D( 64 , 11,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network4-filter2')(input_for_4)\n",
    "    conv_11 = BatchNormalization(name='BN11')(conv_11)\n",
    "    conv_11 = Dropout(drop_out)(conv_11)\n",
    "    conv_12 = Conv1D( 64 , 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network4-filter3')(input_for_4)\n",
    "    conv_12 = BatchNormalization(name='BN12')(conv_12)\n",
    "    conv_12 = Dropout(drop_out)(conv_12)\n",
    "\n",
    "    merge_4 = concatenate([conv_10, conv_11, conv_12],name='Network4')\n",
    "    input_for_5 = concatenate([conv1_input, merge_1, merge_2, merge_3, merge_4],name='Network1234-and-input')\n",
    "\n",
    "\n",
    "\n",
    "    conv_13 = Conv1D( 64 , 19,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network5-filter1')(input_for_5)\n",
    "    conv_13 = BatchNormalization(name='BN13')(conv_13)\n",
    "    conv_13 = Dropout(drop_out)(conv_13)\n",
    "    conv_14 = Conv1D( 64 , 11,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network5-filter2')(input_for_5)\n",
    "    conv_14 = BatchNormalization(name='BN14')(conv_14)\n",
    "    conv_14 = Dropout(drop_out)(conv_14)\n",
    "    conv_15 = Conv1D( 64 , 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg, name='Network5-filter3')(input_for_5)\n",
    "    conv_15 = BatchNormalization(name='BN15')(conv_15)\n",
    "    conv_15 = Dropout(drop_out)(conv_15)\n",
    "\n",
    "    merge_5 = concatenate([conv_13, conv_14, conv_15],name='Network5')\n",
    "\n",
    "\n",
    "\n",
    "    merge_final = concatenate([merge_1, merge_2, merge_3,merge_4, merge_5], name='Final')\n",
    "\n",
    "\n",
    "\n",
    "    flatten  = Flatten()(merge_final)\n",
    "    first_dense = Dense(128, activation='relu', use_bias=True,  kernel_regularizer=w_reg, name='last')(flatten)\n",
    "    first_dense = BatchNormalization(name='BN16')(first_dense)\n",
    "\n",
    "    #second_dense = Dense(32, activation='relu', use_bias=True,  kernel_regularizer=w_reg, name='last')(flatten)\n",
    "    #second_dense = BatchNormalization(name='BN16')(second_dense)\n",
    "\n",
    "    final_model_output = Dense(classSize, activation = 'softmax', name='softmax')(first_dense)\n",
    "\n",
    "    m = Model(inputs=conv1_input, outputs=final_model_output)\n",
    "\n",
    "    opt = Adam(lr=LR)\n",
    "    m.compile(optimizer=opt, loss=loss,metrics=['accuracy', 'mae'])\n",
    "\n",
    "    print(\"\\nHyper Parameters\\n\")\n",
    "    print(\"Learning Rate: \" + str(LR))\n",
    "    print(\"Drop out: \" + str(drop_out))\n",
    "    print(\"Batch dim: \" + str(batch_dim))\n",
    "    print(\"Number of epochs: \" + str(nn_epochs))\n",
    "    print(\"Regularizers: \" + str(w_reg.l2))\n",
    "    print(\"\\nLoss: \" + loss + \"\\n\")\n",
    "    m.summary()\n",
    "\n",
    "    #import os\n",
    "    #os.environ[\"PATH\"] += os.pathsep + 'C:/Users/Ieremie/Anaconda3/pkgs/graphviz-2.38-hfd603c8_2/Library/bin'\n",
    "    #from keras.utils import plot_model\n",
    "    #plot_model(m)#, to_file='model.png')\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from time import time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "nr_models_to_train = 10\n",
    "nn_epochs = 30\n",
    "batch_dim = 64\n",
    "\n",
    "for i in range(2,nr_models_to_train):\n",
    "    \n",
    "    \n",
    "    x_train_final,y_train_final,x_valid_final,y_valid_final = get_dataset()\n",
    "    \n",
    "    #callbacks\n",
    "    filepath=\"//filestore.soton.ac.uk/users/ii1g17/model-assembly-shuffled-\" + str(i) + \".hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max', period=1)\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    m = get_model()\n",
    "\n",
    "    #training \n",
    "    start_time = timer()\n",
    "    history = m.fit(x_train_final, y_train_final, epochs=nn_epochs, batch_size=batch_dim,\n",
    "                validation_data=(x_valid_final, y_valid_final) ,shuffle=True,  callbacks=callbacks_list)\n",
    "\n",
    "    end_time = timer()\n",
    "    print(\"\\n\\nTime elapsed: \" + \"{0:.2f}\".format((end_time - start_time)) + \" s\")\n",
    "    \n",
    "    \n",
    "    #saving results\n",
    "    with open('//filestore.soton.ac.uk/users/ii1g17/model-assembly-shuffled-hist-' + str(i), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "accuracyName = 'accuracyMiddlewindowQ2W19Model2.png'\n",
    "lossName = 'lossMiddlewindowQ2W19Model2.png'\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "#plt.savefig(accuracyName)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "#plt.savefig(lossName)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.GzipFile('cb513_window19Q8.npy.gz', \"r\")\n",
    "X_test_window = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_final = X_test_window[:,:,(21+classSize):]\n",
    "#x_test_final = X_test_window[:,:,0:21]\n",
    "y_test_final = X_test_window[:,:,21: (21+classSize)]\n",
    "print(x_test_final.shape)\n",
    "print(y_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_final = np.reshape(y_test_final, (80119,19*8))\n",
    "print(y_test_final.shape)\n",
    "y_test_final = y_test_final[:,0:8]\n",
    "print(y_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test_final, y_test_final)\n",
    "print(\"Loss: \" + str(scores[0]) + \", Accuracy: \" + str(scores[1]) + \", MAE: \" + str(scores[2]))\n",
    "print(\"yes boi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 3\n",
    "m.save('model_' + str(fold) + '.h5')  # creates a HDF5 file \n",
    "with open('model_scores_' + str(fold), 'wb') as file_pi:\n",
    "    pickle.dump(scores, file_pi)\n",
    "with open('model_history_' + str(fold), 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
