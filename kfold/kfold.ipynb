{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import gzip\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Conv1D, AveragePooling1D, MaxPooling1D, TimeDistributed, LeakyReLU, BatchNormalization, Flatten\n",
    "from tensorflow.keras import optimizers, callbacks\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate, Conv1D, BatchNormalization, Flatten\n",
    "from keras.optimizers import Adagrad, Adam\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.utils import plot_model\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import KFold\n",
    "from time import time\n",
    "from timeit import default_timer as timer\n",
    "import pickle\n",
    "import gzip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('all_dataset_window19MiddleQ2.npy') # load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.GzipFile('all_dataset_window9RightSideQ8.npy.gz', \"r\")\n",
    "dataset = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1161181, 9, 50)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.GzipFile('all_dataset_window19Middle.npy.gz', \"w\")\n",
    "np.save(f, dataset)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSize = 9\n",
    "#predictionIndex = 9\n",
    "classSize = 8  # 2 or 3 \n",
    "#numberOfFeatures = 50 #44 \n",
    "\n",
    "amino_acid_residues = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0005\n",
    "drop_out = 0.3\n",
    "batch_dim = 64\n",
    "nn_epochs = 5\n",
    "w_reg = regularizers.l2(0.0001)\n",
    "number_filters = 16\n",
    "\n",
    "\n",
    "def get_model():\n",
    "   \n",
    "    LR = 0.0005\n",
    "    drop_out = 0.3\n",
    "    batch_dim = 64\n",
    "    nn_epochs = 5\n",
    "    w_reg = regularizers.l2(0.0001)\n",
    "    number_filters = 16\n",
    "\n",
    "    loss = 'categorical_crossentropy'\n",
    "\n",
    "\n",
    "    input_shape = (windowSize, 21)\n",
    "\n",
    "    conv1_input = Input(shape=(windowSize, 21), name='InputWindow')\n",
    "\n",
    "    conv_1 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg)(conv1_input)\n",
    "    conv_1 = BatchNormalization(name='BN1')(conv_1)\n",
    "    conv_2 = Conv1D( 64, 3, strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_1)\n",
    "    conv_2 = BatchNormalization(name='BN2')(conv_2)\n",
    "    conv_3 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_2)\n",
    "    conv_3 = BatchNormalization(name='BN3')(conv_3)\n",
    "\n",
    "\n",
    "\n",
    "    conv_4 = Conv1D( 64, 3, strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg)(conv_3)\n",
    "    conv_4 = BatchNormalization(name='BN4')(conv_4)\n",
    "    conv_5 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_4)\n",
    "    conv_5 = BatchNormalization(name='BN5')(conv_5)\n",
    "    #conv_6 = Conv1D( 73, 3, strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_5)\n",
    "    #conv_6 = BatchNormalization(name='BN6')(conv_6)\n",
    "\n",
    "\n",
    "    #conv_7 = Conv1D( 73, 3,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg)(conv_6)\n",
    "    #conv_7 = BatchNormalization(name='BN7')(conv_7)\n",
    "    #conv_8 = Conv1D( 4, 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_7)\n",
    "    #conv_8 = BatchNormalization(name='BN8')(conv_8)\n",
    "    #conv_9 = Conv1D( 4, 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_8)\n",
    "    #conv_9 = BatchNormalization(name='BN9')(conv_9)\n",
    "\n",
    "\n",
    "    flatten  = Flatten()(conv_5)\n",
    "    first_dense = Dense(16, activation='relu', use_bias=True,  kernel_regularizer=w_reg, name='last')(flatten)\n",
    "    first_dense = BatchNormalization(name='BN10')(first_dense)\n",
    "    final_model_output = Dense(classSize, activation = 'softmax', name='softmax')(first_dense)\n",
    "\n",
    "    m = Model(inputs=conv1_input, outputs=final_model_output)\n",
    "\n",
    "    opt = Adam(lr=LR)\n",
    "    m.compile(optimizer=opt, loss=loss,metrics=['accuracy', 'mae'])\n",
    "\n",
    "    print(\"\\nHyper Parameters\\n\")\n",
    "    print(\"Learning Rate: \" + str(LR))\n",
    "    print(\"Drop out: \" + str(drop_out))\n",
    "    print(\"Batch dim: \" + str(batch_dim))\n",
    "    print(\"Number of epochs: \" + str(nn_epochs))\n",
    "    print(\"Regularizers: \" + str(w_reg.l2))\n",
    "    print(\"\\nLoss: \" + loss + \"\\n\")\n",
    "    m.summary()\n",
    "\n",
    "    #import os\n",
    "    #os.environ[\"PATH\"] += os.pathsep + 'C:/Users/Ieremie/Anaconda3/pkgs/graphviz-2.38-hfd603c8_2/Library/bin'\n",
    "\n",
    "    #plot_model(m)#, to_file='model.png')\n",
    "    return m\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1161181, 9, 50)\n",
      "[[0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.         0.         0.41095954\n",
      "  0.19940776 0.02436359 0.4750208  0.07585818 0.0099518  0.90378445\n",
      "  0.80059224 0.09708863 0.63876313 0.3015348  0.03106848 0.15842418\n",
      "  0.99356788 0.14430314 0.09621554 0.12786157 0.94213307 0.00966052\n",
      "  0.26894143 0.04653048]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.         0.         0.6942364\n",
      "  0.01365366 0.62010646 0.39174098 0.02436359 0.05215356 0.15709548\n",
      "  0.15577585 0.93213767 0.08166025 0.08471056 0.20100899 0.0295981\n",
      "  0.86294872 0.99062234 0.56954622 0.33626127 0.07242649 0.03661483\n",
      "  0.26894143 0.04833763]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "dataset = shuffle(dataset, random_state=111)\n",
    "print(dataset.shape)\n",
    "print(dataset[0,:2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, random_state=10, shuffle=True)\n",
    "for train, test in kf.split(dataset):\n",
    "    print(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Hyper Parameters\n",
      "\n",
      "Learning Rate: 0.0005\n",
      "Drop out: 0.3\n",
      "Batch dim: 64\n",
      "Number of epochs: 5\n",
      "Regularizers: 1e-04\n",
      "\n",
      "Loss: categorical_crossentropy\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "InputWindow (InputLayer)     (None, 9, 21)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 9, 64)             4096      \n",
      "_________________________________________________________________\n",
      "BN1 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN2 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN3 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN4 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN5 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "flatten_32 (Flatten)         (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "last (Dense)                 (None, 16)                9232      \n",
      "_________________________________________________________________\n",
      "BN10 (BatchNormalization)    (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 8)                 136       \n",
      "=================================================================\n",
      "Total params: 64,216\n",
      "Trainable params: 63,544\n",
      "Non-trainable params: 672\n",
      "_________________________________________________________________\n",
      "Train on 1020152 samples, validate on 113350 samples\n",
      "Epoch 1/5\n",
      "1020152/1020152 [==============================] - 251s 246us/step - loss: 1.2294 - acc: 0.5691 - mean_absolute_error: 0.1391 - val_loss: 1.1657 - val_acc: 0.5890 - val_mean_absolute_error: 0.1345\n",
      "Epoch 2/5\n",
      "1020152/1020152 [==============================] - 230s 225us/step - loss: 1.1645 - acc: 0.5896 - mean_absolute_error: 0.1331 - val_loss: 1.1496 - val_acc: 0.5944 - val_mean_absolute_error: 0.1311\n",
      "Epoch 3/5\n",
      "1020152/1020152 [==============================] - 229s 225us/step - loss: 1.1491 - acc: 0.5958 - mean_absolute_error: 0.1313 - val_loss: 1.1346 - val_acc: 0.6006 - val_mean_absolute_error: 0.1297\n",
      "Epoch 4/5\n",
      "1020152/1020152 [==============================] - 229s 224us/step - loss: 1.1407 - acc: 0.5993 - mean_absolute_error: 0.1302 - val_loss: 1.1336 - val_acc: 0.6012 - val_mean_absolute_error: 0.1303\n",
      "Epoch 5/5\n",
      "1020152/1020152 [==============================] - 226s 221us/step - loss: 1.1353 - acc: 0.6019 - mean_absolute_error: 0.1295 - val_loss: 1.1309 - val_acc: 0.6019 - val_mean_absolute_error: 0.1287\n",
      "\n",
      "\n",
      "Time elapsed: 1173.00 s\n",
      "113350/113350 [==============================] - 17s 153us/step\n",
      "Loss: 1.130884377097158, Accuracy: 0.6018614909572122, MAE: 0.12871216541878389 fold 1\n",
      "2\n",
      "\n",
      "Hyper Parameters\n",
      "\n",
      "Learning Rate: 0.0005\n",
      "Drop out: 0.3\n",
      "Batch dim: 64\n",
      "Number of epochs: 5\n",
      "Regularizers: 1e-04\n",
      "\n",
      "Loss: categorical_crossentropy\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "InputWindow (InputLayer)     (None, 9, 21)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 9, 64)             4096      \n",
      "_________________________________________________________________\n",
      "BN1 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN2 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN3 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN4 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN5 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "flatten_33 (Flatten)         (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "last (Dense)                 (None, 16)                9232      \n",
      "_________________________________________________________________\n",
      "BN10 (BatchNormalization)    (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 8)                 136       \n",
      "=================================================================\n",
      "Total params: 64,216\n",
      "Trainable params: 63,544\n",
      "Non-trainable params: 672\n",
      "_________________________________________________________________\n",
      "Train on 1020152 samples, validate on 113350 samples\n",
      "Epoch 1/5\n",
      "1020152/1020152 [==============================] - 241s 236us/step - loss: 1.2288 - acc: 0.5703 - mean_absolute_error: 0.1390 - val_loss: 1.1650 - val_acc: 0.5904 - val_mean_absolute_error: 0.1316\n",
      "Epoch 2/5\n",
      "1020152/1020152 [==============================] - 232s 227us/step - loss: 1.1650 - acc: 0.5898 - mean_absolute_error: 0.1332 - val_loss: 1.1513 - val_acc: 0.5947 - val_mean_absolute_error: 0.1321\n",
      "Epoch 3/5\n",
      "1020152/1020152 [==============================] - 234s 229us/step - loss: 1.1505 - acc: 0.5954 - mean_absolute_error: 0.1315 - val_loss: 1.1379 - val_acc: 0.6007 - val_mean_absolute_error: 0.1310\n",
      "Epoch 4/5\n",
      "1020152/1020152 [==============================] - 231s 227us/step - loss: 1.1417 - acc: 0.5991 - mean_absolute_error: 0.1304 - val_loss: 1.1325 - val_acc: 0.6020 - val_mean_absolute_error: 0.1301\n",
      "Epoch 5/5\n",
      "1020152/1020152 [==============================] - 220s 216us/step - loss: 1.1359 - acc: 0.6016 - mean_absolute_error: 0.1296 - val_loss: 1.1313 - val_acc: 0.6028 - val_mean_absolute_error: 0.1294\n",
      "\n",
      "\n",
      "Time elapsed: 1165.44 s\n",
      "113350/113350 [==============================] - 15s 131us/step\n",
      "Loss: 1.1313497506041963, Accuracy: 0.6028231142479047, MAE: 0.12943652548898302 fold 2\n",
      "3\n",
      "\n",
      "Hyper Parameters\n",
      "\n",
      "Learning Rate: 0.0005\n",
      "Drop out: 0.3\n",
      "Batch dim: 64\n",
      "Number of epochs: 5\n",
      "Regularizers: 1e-04\n",
      "\n",
      "Loss: categorical_crossentropy\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "InputWindow (InputLayer)     (None, 9, 21)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 9, 64)             4096      \n",
      "_________________________________________________________________\n",
      "BN1 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN2 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN3 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN4 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN5 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "flatten_34 (Flatten)         (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "last (Dense)                 (None, 16)                9232      \n",
      "_________________________________________________________________\n",
      "BN10 (BatchNormalization)    (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 8)                 136       \n",
      "=================================================================\n",
      "Total params: 64,216\n",
      "Trainable params: 63,544\n",
      "Non-trainable params: 672\n",
      "_________________________________________________________________\n",
      "Train on 1020152 samples, validate on 113350 samples\n",
      "Epoch 1/5\n",
      "1020152/1020152 [==============================] - 247s 242us/step - loss: 1.2285 - acc: 0.5692 - mean_absolute_error: 0.1391 - val_loss: 1.1746 - val_acc: 0.5846 - val_mean_absolute_error: 0.1380\n",
      "Epoch 2/5\n",
      "1020152/1020152 [==============================] - 220s 216us/step - loss: 1.1636 - acc: 0.5905 - mean_absolute_error: 0.1330 - val_loss: 1.1437 - val_acc: 0.5963 - val_mean_absolute_error: 0.1299\n",
      "Epoch 3/5\n",
      "1020152/1020152 [==============================] - 221s 217us/step - loss: 1.1481 - acc: 0.5959 - mean_absolute_error: 0.1312 - val_loss: 1.1407 - val_acc: 0.5988 - val_mean_absolute_error: 0.1293\n",
      "Epoch 4/5\n",
      "1020152/1020152 [==============================] - 231s 227us/step - loss: 1.1398 - acc: 0.5999 - mean_absolute_error: 0.1301 - val_loss: 1.1325 - val_acc: 0.6021 - val_mean_absolute_error: 0.1313\n",
      "Epoch 5/5\n",
      "1020152/1020152 [==============================] - 232s 227us/step - loss: 1.1343 - acc: 0.6022 - mean_absolute_error: 0.1293 - val_loss: 1.1334 - val_acc: 0.6018 - val_mean_absolute_error: 0.1319\n",
      "\n",
      "\n",
      "Time elapsed: 1162.80 s\n",
      "113350/113350 [==============================] - 18s 162us/step\n",
      "Loss: 1.1333852070756667, Accuracy: 0.6017732686380168, MAE: 0.13186249176323755 fold 3\n",
      "4\n",
      "\n",
      "Hyper Parameters\n",
      "\n",
      "Learning Rate: 0.0005\n",
      "Drop out: 0.3\n",
      "Batch dim: 64\n",
      "Number of epochs: 5\n",
      "Regularizers: 1e-04\n",
      "\n",
      "Loss: categorical_crossentropy\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "InputWindow (InputLayer)     (None, 9, 21)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 9, 64)             4096      \n",
      "_________________________________________________________________\n",
      "BN1 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN2 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN3 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN4 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN5 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "flatten_35 (Flatten)         (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "last (Dense)                 (None, 16)                9232      \n",
      "_________________________________________________________________\n",
      "BN10 (BatchNormalization)    (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 8)                 136       \n",
      "=================================================================\n",
      "Total params: 64,216\n",
      "Trainable params: 63,544\n",
      "Non-trainable params: 672\n",
      "_________________________________________________________________\n",
      "Train on 1020152 samples, validate on 113350 samples\n",
      "Epoch 1/5\n",
      "1020152/1020152 [==============================] - 260s 255us/step - loss: 1.2258 - acc: 0.5708 - mean_absolute_error: 0.1388 - val_loss: 1.1672 - val_acc: 0.5872 - val_mean_absolute_error: 0.1340\n",
      "Epoch 2/5\n",
      "1020152/1020152 [==============================] - 243s 238us/step - loss: 1.1622 - acc: 0.5911 - mean_absolute_error: 0.1328 - val_loss: 1.1487 - val_acc: 0.5932 - val_mean_absolute_error: 0.1320\n",
      "Epoch 3/5\n",
      "1020152/1020152 [==============================] - 235s 230us/step - loss: 1.1471 - acc: 0.5966 - mean_absolute_error: 0.1311 - val_loss: 1.1455 - val_acc: 0.5976 - val_mean_absolute_error: 0.1347\n",
      "Epoch 4/5\n",
      "1020152/1020152 [==============================] - 233s 228us/step - loss: 1.1390 - acc: 0.6001 - mean_absolute_error: 0.1300 - val_loss: 1.1333 - val_acc: 0.6020 - val_mean_absolute_error: 0.1288\n",
      "Epoch 5/5\n",
      "1020152/1020152 [==============================] - 236s 231us/step - loss: 1.1327 - acc: 0.6032 - mean_absolute_error: 0.1291 - val_loss: 1.1298 - val_acc: 0.6044 - val_mean_absolute_error: 0.1302\n",
      "\n",
      "\n",
      "Time elapsed: 1212.69 s\n",
      "113350/113350 [==============================] - 18s 162us/step\n",
      "Loss: 1.1297865649693644, Accuracy: 0.6043670048511759, MAE: 0.13024511045830126 fold 4\n",
      "5\n",
      "\n",
      "Hyper Parameters\n",
      "\n",
      "Learning Rate: 0.0005\n",
      "Drop out: 0.3\n",
      "Batch dim: 64\n",
      "Number of epochs: 5\n",
      "Regularizers: 1e-04\n",
      "\n",
      "Loss: categorical_crossentropy\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "InputWindow (InputLayer)     (None, 9, 21)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 9, 64)             4096      \n",
      "_________________________________________________________________\n",
      "BN1 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN2 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN3 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_179 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN4 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_180 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN5 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "flatten_36 (Flatten)         (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "last (Dense)                 (None, 16)                9232      \n",
      "_________________________________________________________________\n",
      "BN10 (BatchNormalization)    (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 8)                 136       \n",
      "=================================================================\n",
      "Total params: 64,216\n",
      "Trainable params: 63,544\n",
      "Non-trainable params: 672\n",
      "_________________________________________________________________\n",
      "Train on 1020152 samples, validate on 113350 samples\n",
      "Epoch 1/5\n",
      "1020152/1020152 [==============================] - 257s 252us/step - loss: 1.2324 - acc: 0.5690 - mean_absolute_error: 0.1392 - val_loss: 1.1720 - val_acc: 0.5879 - val_mean_absolute_error: 0.1333.5689 - mean\n",
      "Epoch 2/5\n",
      "1020152/1020152 [==============================] - 248s 243us/step - loss: 1.1631 - acc: 0.5901 - mean_absolute_error: 0.1329 - val_loss: 1.1501 - val_acc: 0.5967 - val_mean_absolute_error: 0.1311\n",
      "Epoch 3/5\n",
      "1020152/1020152 [==============================] - 244s 240us/step - loss: 1.1456 - acc: 0.5970 - mean_absolute_error: 0.1310 - val_loss: 1.1414 - val_acc: 0.5991 - val_mean_absolute_error: 0.1287\n",
      "Epoch 4/5\n",
      "1020152/1020152 [==============================] - 239s 235us/step - loss: 1.1364 - acc: 0.6007 - mean_absolute_error: 0.1298 - val_loss: 1.1386 - val_acc: 0.6014 - val_mean_absolute_error: 0.1318\n",
      "Epoch 5/5\n",
      "1020152/1020152 [==============================] - 232s 228us/step - loss: 1.1308 - acc: 0.6033 - mean_absolute_error: 0.1290 - val_loss: 1.1342 - val_acc: 0.6031 - val_mean_absolute_error: 0.1274\n",
      "\n",
      "\n",
      "Time elapsed: 1224.40 s\n",
      "113350/113350 [==============================] - 18s 155us/step\n",
      "Loss: 1.1341684879665601, Accuracy: 0.6031230701372704, MAE: 0.12743044415995788 fold 5\n",
      "6\n",
      "\n",
      "Hyper Parameters\n",
      "\n",
      "Learning Rate: 0.0005\n",
      "Drop out: 0.3\n",
      "Batch dim: 64\n",
      "Number of epochs: 5\n",
      "Regularizers: 1e-04\n",
      "\n",
      "Loss: categorical_crossentropy\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "InputWindow (InputLayer)     (None, 9, 21)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_181 (Conv1D)          (None, 9, 64)             4096      \n",
      "_________________________________________________________________\n",
      "BN1 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_182 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN2 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_183 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN3 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_184 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN4 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_185 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN5 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "flatten_37 (Flatten)         (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "last (Dense)                 (None, 16)                9232      \n",
      "_________________________________________________________________\n",
      "BN10 (BatchNormalization)    (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 8)                 136       \n",
      "=================================================================\n",
      "Total params: 64,216\n",
      "Trainable params: 63,544\n",
      "Non-trainable params: 672\n",
      "_________________________________________________________________\n",
      "Train on 1020152 samples, validate on 113350 samples\n",
      "Epoch 1/5\n",
      "1020152/1020152 [==============================] - 249s 244us/step - loss: 1.2284 - acc: 0.5697 - mean_absolute_error: 0.1390 - val_loss: 1.1662 - val_acc: 0.5883 - val_mean_absolute_error: 0.1332\n",
      "Epoch 2/5\n",
      "1020152/1020152 [==============================] - 246s 241us/step - loss: 1.1623 - acc: 0.5907 - mean_absolute_error: 0.1329 - val_loss: 1.1440 - val_acc: 0.5972 - val_mean_absolute_error: 0.1315\n",
      "Epoch 3/5\n",
      "1020152/1020152 [==============================] - 248s 243us/step - loss: 1.1471 - acc: 0.5967 - mean_absolute_error: 0.1312 - val_loss: 1.1366 - val_acc: 0.5994 - val_mean_absolute_error: 0.1314\n",
      "Epoch 4/5\n",
      "1020152/1020152 [==============================] - 247s 242us/step - loss: 1.1382 - acc: 0.5997 - mean_absolute_error: 0.1301 - val_loss: 1.1385 - val_acc: 0.5999 - val_mean_absolute_error: 0.1320\n",
      "Epoch 5/5\n",
      "1020152/1020152 [==============================] - 249s 244us/step - loss: 1.1321 - acc: 0.6030 - mean_absolute_error: 0.1292 - val_loss: 1.1314 - val_acc: 0.6038 - val_mean_absolute_error: 0.1310\n",
      "\n",
      "\n",
      "Time elapsed: 1242.14 s\n",
      "113350/113350 [==============================] - 20s 179us/step\n",
      "Loss: 1.1313974950278862, Accuracy: 0.603793559770622, MAE: 0.13095134723249593 fold 6\n",
      "7\n",
      "\n",
      "Hyper Parameters\n",
      "\n",
      "Learning Rate: 0.0005\n",
      "Drop out: 0.3\n",
      "Batch dim: 64\n",
      "Number of epochs: 5\n",
      "Regularizers: 1e-04\n",
      "\n",
      "Loss: categorical_crossentropy\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "InputWindow (InputLayer)     (None, 9, 21)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_186 (Conv1D)          (None, 9, 64)             4096      \n",
      "_________________________________________________________________\n",
      "BN1 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_187 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN2 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_188 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN3 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_189 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN4 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_190 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN5 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "flatten_38 (Flatten)         (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "last (Dense)                 (None, 16)                9232      \n",
      "_________________________________________________________________\n",
      "BN10 (BatchNormalization)    (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 8)                 136       \n",
      "=================================================================\n",
      "Total params: 64,216\n",
      "Trainable params: 63,544\n",
      "Non-trainable params: 672\n",
      "_________________________________________________________________\n",
      "Train on 1020152 samples, validate on 113350 samples\n",
      "Epoch 1/5\n",
      "1020152/1020152 [==============================] - 267s 261us/step - loss: 1.2297 - acc: 0.5695 - mean_absolute_error: 0.1391 - val_loss: 1.1672 - val_acc: 0.5901 - val_mean_absolute_error: 0.1346\n",
      "Epoch 2/5\n",
      "1020152/1020152 [==============================] - 252s 247us/step - loss: 1.1637 - acc: 0.5903 - mean_absolute_error: 0.1330 - val_loss: 1.1506 - val_acc: 0.5950 - val_mean_absolute_error: 0.1319\n",
      "Epoch 3/5\n",
      "1020152/1020152 [==============================] - 251s 246us/step - loss: 1.1485 - acc: 0.5959 - mean_absolute_error: 0.1312 - val_loss: 1.1381 - val_acc: 0.5996 - val_mean_absolute_error: 0.1294\n",
      "Epoch 4/5\n",
      "1020152/1020152 [==============================] - 251s 246us/step - loss: 1.1394 - acc: 0.6001 - mean_absolute_error: 0.1300 - val_loss: 1.1322 - val_acc: 0.6041 - val_mean_absolute_error: 0.1315\n",
      "Epoch 5/5\n",
      "1020152/1020152 [==============================] - 246s 242us/step - loss: 1.1328 - acc: 0.6032 - mean_absolute_error: 0.1291 - val_loss: 1.1322 - val_acc: 0.6027 - val_mean_absolute_error: 0.1295\n",
      "\n",
      "\n",
      "Time elapsed: 1271.00 s\n",
      "113350/113350 [==============================] - 20s 178us/step\n",
      "Loss: 1.132150999926553, Accuracy: 0.6026731363045364, MAE: 0.1294513378111234 fold 7\n",
      "8\n",
      "\n",
      "Hyper Parameters\n",
      "\n",
      "Learning Rate: 0.0005\n",
      "Drop out: 0.3\n",
      "Batch dim: 64\n",
      "Number of epochs: 5\n",
      "Regularizers: 1e-04\n",
      "\n",
      "Loss: categorical_crossentropy\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "InputWindow (InputLayer)     (None, 9, 21)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_191 (Conv1D)          (None, 9, 64)             4096      \n",
      "_________________________________________________________________\n",
      "BN1 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_192 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN2 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_193 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN3 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_194 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN4 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_195 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN5 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "flatten_39 (Flatten)         (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "last (Dense)                 (None, 16)                9232      \n",
      "_________________________________________________________________\n",
      "BN10 (BatchNormalization)    (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 8)                 136       \n",
      "=================================================================\n",
      "Total params: 64,216\n",
      "Trainable params: 63,544\n",
      "Non-trainable params: 672\n",
      "_________________________________________________________________\n",
      "Train on 1020152 samples, validate on 113350 samples\n",
      "Epoch 1/5\n",
      "1020152/1020152 [==============================] - 267s 262us/step - loss: 1.2279 - acc: 0.5700 - mean_absolute_error: 0.1391 - val_loss: 1.1790 - val_acc: 0.5846 - val_mean_absolute_error: 0.1333\n",
      "Epoch 2/5\n",
      "1020152/1020152 [==============================] - 251s 246us/step - loss: 1.1636 - acc: 0.5901 - mean_absolute_error: 0.1330 - val_loss: 1.1517 - val_acc: 0.5958 - val_mean_absolute_error: 0.1330\n",
      "Epoch 3/5\n",
      "1020152/1020152 [==============================] - 250s 245us/step - loss: 1.1481 - acc: 0.5959 - mean_absolute_error: 0.1312 - val_loss: 1.1434 - val_acc: 0.6001 - val_mean_absolute_error: 0.1329\n",
      "Epoch 4/5\n",
      "1020152/1020152 [==============================] - 254s 249us/step - loss: 1.1397 - acc: 0.5997 - mean_absolute_error: 0.1301 - val_loss: 1.1397 - val_acc: 0.6023 - val_mean_absolute_error: 0.1323\n",
      "Epoch 5/5\n",
      "1020152/1020152 [==============================] - 251s 246us/step - loss: 1.1336 - acc: 0.6026 - mean_absolute_error: 0.1292 - val_loss: 1.1390 - val_acc: 0.6027 - val_mean_absolute_error: 0.1306\n",
      "\n",
      "\n",
      "Time elapsed: 1280.39 s\n",
      "113350/113350 [==============================] - 21s 186us/step\n",
      "Loss: 1.138966077822234, Accuracy: 0.6027172474636083, MAE: 0.13060240974129692 fold 8\n",
      "9\n",
      "\n",
      "Hyper Parameters\n",
      "\n",
      "Learning Rate: 0.0005\n",
      "Drop out: 0.3\n",
      "Batch dim: 64\n",
      "Number of epochs: 5\n",
      "Regularizers: 1e-04\n",
      "\n",
      "Loss: categorical_crossentropy\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "InputWindow (InputLayer)     (None, 9, 21)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_196 (Conv1D)          (None, 9, 64)             4096      \n",
      "_________________________________________________________________\n",
      "BN1 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_197 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN2 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_198 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN3 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_199 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN4 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_200 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN5 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "flatten_40 (Flatten)         (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "last (Dense)                 (None, 16)                9232      \n",
      "_________________________________________________________________\n",
      "BN10 (BatchNormalization)    (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 8)                 136       \n",
      "=================================================================\n",
      "Total params: 64,216\n",
      "Trainable params: 63,544\n",
      "Non-trainable params: 672\n",
      "_________________________________________________________________\n",
      "Train on 1020152 samples, validate on 113350 samples\n",
      "Epoch 1/5\n",
      "1020152/1020152 [==============================] - 272s 266us/step - loss: 1.2303 - acc: 0.5703 - mean_absolute_error: 0.1390 - val_loss: 1.1684 - val_acc: 0.5868 - val_mean_absolute_error: 0.1321\n",
      "Epoch 2/5\n",
      "1020152/1020152 [==============================] - 258s 253us/step - loss: 1.1627 - acc: 0.5905 - mean_absolute_error: 0.1329 - val_loss: 1.1432 - val_acc: 0.5974 - val_mean_absolute_error: 0.1323.5905 - mean_absolute_error: 0.1\n",
      "Epoch 3/5\n",
      "1020152/1020152 [==============================] - 252s 247us/step - loss: 1.1481 - acc: 0.5962 - mean_absolute_error: 0.1312 - val_loss: 1.1402 - val_acc: 0.5993 - val_mean_absolute_error: 0.1293\n",
      "Epoch 4/5\n",
      "1020152/1020152 [==============================] - 255s 250us/step - loss: 1.1395 - acc: 0.5998 - mean_absolute_error: 0.1301 - val_loss: 1.1346 - val_acc: 0.6010 - val_mean_absolute_error: 0.1304\n",
      "Epoch 5/5\n",
      "1020152/1020152 [==============================] - 254s 249us/step - loss: 1.1339 - acc: 0.6029 - mean_absolute_error: 0.1293 - val_loss: 1.1328 - val_acc: 0.6020 - val_mean_absolute_error: 0.1269\n",
      "\n",
      "\n",
      "Time elapsed: 1298.80 s\n",
      "113350/113350 [==============================] - 21s 184us/step\n",
      "Loss: 1.1327770637929517, Accuracy: 0.6019585355105356, MAE: 0.12692902390631905 fold 9\n",
      "10\n",
      "\n",
      "Hyper Parameters\n",
      "\n",
      "Learning Rate: 0.0005\n",
      "Drop out: 0.3\n",
      "Batch dim: 64\n",
      "Number of epochs: 5\n",
      "Regularizers: 1e-04\n",
      "\n",
      "Loss: categorical_crossentropy\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "InputWindow (InputLayer)     (None, 9, 21)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_201 (Conv1D)          (None, 9, 64)             4096      \n",
      "_________________________________________________________________\n",
      "BN1 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_202 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN2 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_203 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN3 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_204 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN4 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_205 (Conv1D)          (None, 9, 64)             12352     \n",
      "_________________________________________________________________\n",
      "BN5 (BatchNormalization)     (None, 9, 64)             256       \n",
      "_________________________________________________________________\n",
      "flatten_41 (Flatten)         (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "last (Dense)                 (None, 16)                9232      \n",
      "_________________________________________________________________\n",
      "BN10 (BatchNormalization)    (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 8)                 136       \n",
      "=================================================================\n",
      "Total params: 64,216\n",
      "Trainable params: 63,544\n",
      "Non-trainable params: 672\n",
      "_________________________________________________________________\n",
      "Train on 1020152 samples, validate on 113350 samples\n",
      "Epoch 1/5\n",
      "1020152/1020152 [==============================] - 269s 263us/step - loss: 1.2268 - acc: 0.5703 - mean_absolute_error: 0.1389 - val_loss: 1.1659 - val_acc: 0.5916 - val_mean_absolute_error: 0.1352\n",
      "Epoch 2/5\n",
      "1020152/1020152 [==============================] - 254s 249us/step - loss: 1.1631 - acc: 0.5902 - mean_absolute_error: 0.1329 - val_loss: 1.1454 - val_acc: 0.5968 - val_mean_absolute_error: 0.1326\n",
      "Epoch 3/5\n",
      "1020152/1020152 [==============================] - 251s 246us/step - loss: 1.1476 - acc: 0.5965 - mean_absolute_error: 0.1311 - val_loss: 1.1406 - val_acc: 0.6004 - val_mean_absolute_error: 0.1293\n",
      "Epoch 4/5\n",
      "1020152/1020152 [==============================] - 250s 245us/step - loss: 1.1396 - acc: 0.6002 - mean_absolute_error: 0.1301 - val_loss: 1.1337 - val_acc: 0.6048 - val_mean_absolute_error: 0.1295\n",
      "Epoch 5/5\n",
      "1020152/1020152 [==============================] - 252s 247us/step - loss: 1.1339 - acc: 0.6026 - mean_absolute_error: 0.1293 - val_loss: 1.1338 - val_acc: 0.6031 - val_mean_absolute_error: 0.1293\n",
      "\n",
      "\n",
      "Time elapsed: 1283.29 s\n",
      "113350/113350 [==============================] - 21s 183us/step\n",
      "Loss: 1.1338382429439273, Accuracy: 0.6031230701377963, MAE: 0.12931350257804755 fold 10\n"
     ]
    }
   ],
   "source": [
    "l = 0\n",
    "r = 113350\n",
    "n = 1133502\n",
    "\n",
    "fold = 0\n",
    "valid_indexes = [()]\n",
    "\n",
    "\n",
    "for i in range (10):\n",
    "  \n",
    "    fold += 1\n",
    "    print(fold)\n",
    "    \n",
    "    m = get_model()\n",
    "    start_time = timer()\n",
    "    history = m.fit(np.concatenate((dataset[0:l], dataset[r:n]), axis=0)[:,:,21+classSize:], np.concatenate((dataset[0:l], dataset[r:n]), axis=0)[:, 0, 21 : (21+classSize)],\n",
    "                    epochs=nn_epochs, validation_data=(dataset[l:r][:,:,21+classSize:], dataset[l:r][:, 0, 21 : (21+classSize)]) ,shuffle=True,batch_size=batch_dim)\n",
    "    end_time = timer()\n",
    "    print(\"\\n\\nTime elapsed: \" + \"{0:.2f}\".format((end_time - start_time)) + \" s\")\n",
    "    \n",
    "    scores = m.evaluate(dataset[l:r][:,:,21+classSize:], dataset[l:r][:, 0, 21 : (21+classSize)])\n",
    "    print(\"Loss: \" + str(scores[0]) + \", Accuracy: \" + str(scores[1]) + \", MAE: \" + str(scores[2]) + ' fold ' + str(fold))\n",
    "    \n",
    "    \n",
    "    m.save('model_kfold' + str(fold) + '.h5')  # creates a HDF5 file \n",
    "    with open('model_scores_kfold' + str(fold), 'wb') as file_pi:\n",
    "        pickle.dump(scores, file_pi)\n",
    "    with open('model_history_kfold' + str(fold), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "    \n",
    "    valid_indexes.append((l, r))\n",
    "    r += 113350\n",
    "    l += 113350\n",
    "\n",
    "with open('model_kfold_indexes', 'wb') as file_pi:\n",
    "    pickle.dump(valid_indexes, file_pi)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, random_state=10, shuffle=True)\n",
    "fold = 0\n",
    "valid_indexes = [([],[])]\n",
    "for train, test in kf.split(dataset):\n",
    "\n",
    "    fold += 1\n",
    "    print(fold)\n",
    "    \n",
    "    m = get_model()\n",
    "    start_time = timer()\n",
    "    history = m.fit(dataset[train][:,:,0:21], dataset[train][:,:,21 : (21+classSize)], epochs=nn_epochs,\n",
    "                    batch_size=batch_dim, validation_data=(dataset[test][:,:,0:21], dataset[test][:,:,21 : (21+classSize)]),shuffle=True)\n",
    "    end_time = timer()\n",
    "    print(\"\\n\\nTime elapsed: \" + \"{0:.2f}\".format((end_time - start_time)) + \" s\")\n",
    "\n",
    "    \n",
    "    scores = m.evaluate(dataset[test][:,:,0:21], dataset[test][:,:,21 : (21+classSize)])\n",
    "    print(\"Loss: \" + str(scores[0]) + \", Accuracy: \" + str(scores[1]) + \", MAE: \" + str(scores[2]) + ' fold ' + str(fold))\n",
    "    \n",
    "    \n",
    "    m.save('model_kfold' + str(fold) + '.h5')  # creates a HDF5 file \n",
    "    with open('model_scores_kfold' + str(fold), 'wb') as file_pi:\n",
    "        pickle.dump(scores, file_pi)\n",
    "    with open('model_history_kfold' + str(fold), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "    \n",
    "    valid_indexes.append((train, test))\n",
    "\n",
    "with open('model_kfold_indexes', 'wb') as file_pi:\n",
    "    pickle.dump(valid_indexes, file_pi)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
