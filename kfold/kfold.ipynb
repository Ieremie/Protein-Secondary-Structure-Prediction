{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import gzip\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Conv1D, AveragePooling1D, MaxPooling1D, TimeDistributed, LeakyReLU, BatchNormalization, Flatten\n",
    "from tensorflow.keras import optimizers, callbacks\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate, Conv1D, BatchNormalization, Flatten\n",
    "from keras.optimizers import Adagrad, Adam\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.utils import plot_model\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import KFold\n",
    "from time import time\n",
    "from timeit import default_timer as timer\n",
    "import pickle\n",
    "import gzip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('all_dataset_window19MiddleQ2.npy') # load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.GzipFile('all_dataset_window19MiddleQ3.npy.gz', \"r\")\n",
    "dataset = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1139033, 19, 45)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.GzipFile('all_dataset_window19Middle.npy.gz', \"w\")\n",
    "np.save(f, dataset)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSize = 19\n",
    "predictionIndex = 9\n",
    "classSize = 3  # 2 or 3 \n",
    "numberOfFeatures = 50 #44 \n",
    "\n",
    "amino_acid_residues = 21\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0005\n",
    "drop_out = 0.3\n",
    "batch_dim = 64\n",
    "nn_epochs = 5\n",
    "w_reg = regularizers.l2(0.0001)\n",
    "number_filters = 16\n",
    "\n",
    "\n",
    "def get_model():\n",
    "   \n",
    "    LR = 0.0005\n",
    "    drop_out = 0.3\n",
    "    batch_dim = 64\n",
    "    nn_epochs = 5\n",
    "    w_reg = regularizers.l2(0.0001)\n",
    "    number_filters = 16\n",
    "\n",
    "    loss = 'categorical_crossentropy'\n",
    "\n",
    "\n",
    "    input_shape = (windowSize, 21)\n",
    "\n",
    "    conv1_input = Input(shape=(windowSize, 21), name='InputWindow')\n",
    "\n",
    "    conv_1 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg)(conv1_input)\n",
    "    conv_1 = BatchNormalization(name='BN1')(conv_1)\n",
    "    conv_2 = Conv1D( 64, 3, strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_1)\n",
    "    conv_2 = BatchNormalization(name='BN2')(conv_2)\n",
    "    conv_3 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_2)\n",
    "    conv_3 = BatchNormalization(name='BN3')(conv_3)\n",
    "\n",
    "\n",
    "\n",
    "    conv_4 = Conv1D( 64, 3, strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg)(conv_3)\n",
    "    conv_4 = BatchNormalization(name='BN4')(conv_4)\n",
    "    conv_5 = Conv1D( 64, 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_4)\n",
    "    conv_5 = BatchNormalization(name='BN5')(conv_5)\n",
    "    #conv_6 = Conv1D( 73, 3, strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_5)\n",
    "    #conv_6 = BatchNormalization(name='BN6')(conv_6)\n",
    "\n",
    "\n",
    "    #conv_7 = Conv1D( 73, 3,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg)(conv_6)\n",
    "    #conv_7 = BatchNormalization(name='BN7')(conv_7)\n",
    "    #conv_8 = Conv1D( 4, 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_7)\n",
    "    #conv_8 = BatchNormalization(name='BN8')(conv_8)\n",
    "    #conv_9 = Conv1D( 4, 3,  strides=1, padding='same', activation='relu', use_bias=True,kernel_regularizer=w_reg)(conv_8)\n",
    "    #conv_9 = BatchNormalization(name='BN9')(conv_9)\n",
    "\n",
    "\n",
    "    flatten  = Flatten()(conv_5)\n",
    "    first_dense = Dense(16, activation='relu', use_bias=True,  kernel_regularizer=w_reg, name='last')(flatten)\n",
    "    first_dense = BatchNormalization(name='BN10')(first_dense)\n",
    "    final_model_output = Dense(num_classes, activation = 'softmax', name='softmax')(first_dense)\n",
    "\n",
    "    m = Model(inputs=conv1_input, outputs=final_model_output)\n",
    "\n",
    "    opt = Adam(lr=LR)\n",
    "    m.compile(optimizer=opt, loss=loss,metrics=['accuracy', 'mae'])\n",
    "\n",
    "    print(\"\\nHyper Parameters\\n\")\n",
    "    print(\"Learning Rate: \" + str(LR))\n",
    "    print(\"Drop out: \" + str(drop_out))\n",
    "    print(\"Batch dim: \" + str(batch_dim))\n",
    "    print(\"Number of epochs: \" + str(nn_epochs))\n",
    "    print(\"Regularizers: \" + str(w_reg.l2))\n",
    "    print(\"\\nLoss: \" + loss + \"\\n\")\n",
    "    m.summary()\n",
    "\n",
    "    #import os\n",
    "    #os.environ[\"PATH\"] += os.pathsep + 'C:/Users/Ieremie/Anaconda3/pkgs/graphviz-2.38-hfd603c8_2/Library/bin'\n",
    "\n",
    "    #plot_model(m)#, to_file='model.png')\n",
    "    return m\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1139033, 19, 45)\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.\n",
      "  0.14804719 0.02003571 0.34298956 0.57444251 0.03732689 0.14063814\n",
      "  0.99275368 0.18242553 0.82920456 0.18846732 0.08627421 0.96739054\n",
      "  0.03992533 0.4551211  0.92757356 0.27289176 0.56954622 0.11920292\n",
      "  0.01781043 0.26894143 0.3078905 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.\n",
      "  0.08393843 0.01087881 0.06238585 0.35205919 0.01077173 0.04521748\n",
      "  0.41338244 0.09975048 0.99715686 0.01567329 0.03992533 0.11007258\n",
      "  0.03954378 0.73885    0.81305736 0.81153268 0.54239792 0.07242649\n",
      "  0.0090133  0.26894143 0.21416503]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "dataset = shuffle(dataset, random_state=111)\n",
    "print(dataset.shape)\n",
    "print(dataset[0,:2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, random_state=10, shuffle=True)\n",
    "for train, test in kf.split(dataset):\n",
    "    print(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0320 16:29:04.418337 12764 deprecation_wrapper.py:119] From C:\\Apps\\Anaconda3\\envs\\gpu-cuda10\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0320 16:29:05.048079 12764 deprecation_wrapper.py:119] From C:\\Apps\\Anaconda3\\envs\\gpu-cuda10\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0320 16:29:05.075870 12764 deprecation_wrapper.py:119] From C:\\Apps\\Anaconda3\\envs\\gpu-cuda10\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0320 16:29:05.451344 12764 deprecation_wrapper.py:119] From C:\\Apps\\Anaconda3\\envs\\gpu-cuda10\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0320 16:29:06.255559 12764 deprecation_wrapper.py:119] From C:\\Apps\\Anaconda3\\envs\\gpu-cuda10\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0320 16:29:06.262651 12764 deprecation_wrapper.py:119] From C:\\Apps\\Anaconda3\\envs\\gpu-cuda10\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper Parameters\n",
      "\n",
      "Learning Rate: 0.0005\n",
      "Drop out: 0.3\n",
      "Batch dim: 64\n",
      "Number of epochs: 5\n",
      "Regularizers: 1e-04\n",
      "\n",
      "Loss: categorical_crossentropy\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "InputWindow (InputLayer)     (None, 19, 21)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 19, 64)            4096      \n",
      "_________________________________________________________________\n",
      "BN1 (BatchNormalization)     (None, 19, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 19, 64)            12352     \n",
      "_________________________________________________________________\n",
      "BN2 (BatchNormalization)     (None, 19, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 19, 64)            12352     \n",
      "_________________________________________________________________\n",
      "BN3 (BatchNormalization)     (None, 19, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 19, 64)            12352     \n",
      "_________________________________________________________________\n",
      "BN4 (BatchNormalization)     (None, 19, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 19, 64)            12352     \n",
      "_________________________________________________________________\n",
      "BN5 (BatchNormalization)     (None, 19, 64)            256       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1216)              0         \n",
      "_________________________________________________________________\n",
      "last (Dense)                 (None, 16)                19472     \n",
      "_________________________________________________________________\n",
      "BN10 (BatchNormalization)    (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 74,371\n",
      "Trainable params: 73,699\n",
      "Non-trainable params: 672\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0320 16:29:30.185673 12764 deprecation.py:323] From C:\\Apps\\Anaconda3\\envs\\gpu-cuda10\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1020152 samples, validate on 113350 samples\n",
      "Epoch 1/5\n",
      " 343360/1020152 [=========>....................] - ETA: 5:52 - loss: 0.8208 - acc: 0.6526 - mean_absolute_error: 0.3027"
     ]
    }
   ],
   "source": [
    "l = 0\n",
    "r = 113350\n",
    "n = 1133502\n",
    "\n",
    "fold = 0\n",
    "valid_indexes = [()]\n",
    "\n",
    "\n",
    "for i in range (10):\n",
    "  \n",
    "    fold += 1\n",
    "    print(fold)\n",
    "    \n",
    "    m = get_model()\n",
    "    start_time = timer()\n",
    "    history = m.fit(np.concatenate((dataset[0:l], dataset[r:n]), axis=0)[:,:,0:21], np.concatenate((dataset[0:l], dataset[r:n]), axis=0)[:, 0, 21 : (21+classSize)],\n",
    "                    epochs=nn_epochs, validation_data=(dataset[l:r][:,:,0:21], dataset[l:r][:, 0, 21 : (21+classSize)]) ,shuffle=True,batch_size=batch_dim)\n",
    "    end_time = timer()\n",
    "    print(\"\\n\\nTime elapsed: \" + \"{0:.2f}\".format((end_time - start_time)) + \" s\")\n",
    "    \n",
    "    scores = m.evaluate(dataset[l:r][:,:,0:21], dataset[l:r][:, 0, 21 : (21+classSize)])\n",
    "    print(\"Loss: \" + str(scores[0]) + \", Accuracy: \" + str(scores[1]) + \", MAE: \" + str(scores[2]) + ' fold ' + str(fold))\n",
    "    \n",
    "    \n",
    "    m.save('model_kfold' + str(fold) + '.h5')  # creates a HDF5 file \n",
    "    with open('model_scores_kfold' + str(fold), 'wb') as file_pi:\n",
    "        pickle.dump(scores, file_pi)\n",
    "    with open('model_history_kfold' + str(fold), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "    \n",
    "    valid_indexes.append((l, r))\n",
    "    r += 113350\n",
    "    l += 113350\n",
    "\n",
    "with open('model_kfold_indexes', 'wb') as file_pi:\n",
    "    pickle.dump(valid_indexes, file_pi)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, random_state=10, shuffle=True)\n",
    "fold = 0\n",
    "valid_indexes = [([],[])]\n",
    "for train, test in kf.split(dataset):\n",
    "\n",
    "    fold += 1\n",
    "    print(fold)\n",
    "    \n",
    "    m = get_model()\n",
    "    start_time = timer()\n",
    "    history = m.fit(dataset[train][:,:,0:21], dataset[train][:,:,21 : (21+classSize)], epochs=nn_epochs,\n",
    "                    batch_size=batch_dim, validation_data=(dataset[test][:,:,0:21], dataset[test][:,:,21 : (21+classSize)]),shuffle=True)\n",
    "    end_time = timer()\n",
    "    print(\"\\n\\nTime elapsed: \" + \"{0:.2f}\".format((end_time - start_time)) + \" s\")\n",
    "\n",
    "    \n",
    "    scores = m.evaluate(dataset[test][:,:,0:21], dataset[test][:,:,21 : (21+classSize)])\n",
    "    print(\"Loss: \" + str(scores[0]) + \", Accuracy: \" + str(scores[1]) + \", MAE: \" + str(scores[2]) + ' fold ' + str(fold))\n",
    "    \n",
    "    \n",
    "    m.save('model_kfold' + str(fold) + '.h5')  # creates a HDF5 file \n",
    "    with open('model_scores_kfold' + str(fold), 'wb') as file_pi:\n",
    "        pickle.dump(scores, file_pi)\n",
    "    with open('model_history_kfold' + str(fold), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "    \n",
    "    valid_indexes.append((train, test))\n",
    "\n",
    "with open('model_kfold_indexes', 'wb') as file_pi:\n",
    "    pickle.dump(valid_indexes, file_pi)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
