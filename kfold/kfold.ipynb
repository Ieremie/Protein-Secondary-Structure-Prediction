{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import KFold\n",
    "from time import time\n",
    "from timeit import default_timer as timer\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('all_dataset_window19Middle.npy') # load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSize = 19\n",
    "predictionIndex = 9\n",
    "classSize = 8  # 2 or 3 \n",
    "numberOfFeatures = 50 #44 \n",
    "\n",
    "amino_acid_residues = 21\n",
    "num_classes = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv1D, AveragePooling1D, MaxPooling1D, TimeDistributed, LeakyReLU, BatchNormalization, Flatten\n",
    "from keras import optimizers, callbacks\n",
    "from keras import regularizers\n",
    "from keras.utils import plot_model\n",
    "import os\n",
    "\n",
    "LR = 0.0005\n",
    "drop_out = 0.3\n",
    "batch_dim = 64\n",
    "nn_epochs = 10\n",
    "w_reg = regularizers.l2(0.0001)\n",
    "number_filters = 16\n",
    "\n",
    "\n",
    "def get_model():\n",
    "   \n",
    "\n",
    "    loss = 'categorical_crossentropy'\n",
    "\n",
    "\n",
    "    m = Sequential()\n",
    "\n",
    "#first convolutional neural netwok\n",
    "    m.add(Conv1D( 16 , 3,  strides=1, padding='same', activation='relu', use_bias=True, input_shape=(windowSize, 21), kernel_regularizer=w_reg))\n",
    "    m.add(BatchNormalization())\n",
    "\n",
    "    m.add(Conv1D( 16, 3,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Conv1D( 16, 5,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Conv1D( 16, 7,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "    m.add(BatchNormalization())\n",
    "\n",
    "\n",
    "    m.add(Conv1D( 16, 3,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Conv1D( 16, 5,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Conv1D( 16, 7,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "    m.add(BatchNormalization())\n",
    "\n",
    "    m.add(Conv1D( 16, 3,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Conv1D( 16, 5,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Conv1D( 16, 7,  strides=1, padding='same', activation='relu', use_bias=True, kernel_regularizer=w_reg))\n",
    "    m.add(BatchNormalization())\n",
    "\n",
    "\n",
    "    m.add(Flatten())\n",
    "\n",
    "    #4 dense layer\n",
    "    m.add(Dense(200, activation='relu', use_bias=True,  kernel_regularizer=w_reg))\n",
    "\n",
    "    #5 softmax output layer\n",
    "    m.add(Dense(num_classes, activation = 'softmax'))\n",
    "\n",
    "    opt = optimizers.Adam(lr=LR)\n",
    "    m.compile(optimizer=opt, loss=loss,metrics=['accuracy', 'mae'])\n",
    "\n",
    "    print(\"\\nHyper Parameters\\n\")\n",
    "    print(\"Learning Rate: \" + str(LR))\n",
    "    print(\"Drop out: \" + str(drop_out))\n",
    "\n",
    "    print(\"Batch dim: \" + str(batch_dim))\n",
    "    print(\"Number of epochs: \" + str(nn_epochs))\n",
    "    print(\"Regularizers: \" + str(w_reg.l2))\n",
    "    print(\"\\nLoss: \" + loss + \"\\n\")\n",
    "    m.summary()\n",
    "\n",
    "    #os.environ[\"PATH\"] += os.pathsep + 'C:/Users/Ieremie/Anaconda3/pkgs/graphviz-2.38-hfd603c8_2/Library/bin'\n",
    "    #plot_model(m)\n",
    "    \n",
    "    return m\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1133502, 19, 50)\n",
      "[[1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.99043465\n",
      "  0.02297737 0.2158528  0.77729988 0.02412702 0.19309869 0.08706578\n",
      "  0.01964677 0.22270013 0.0511737  0.04148712 0.12346705 0.03768789\n",
      "  0.38936076 0.86875552 0.82200629 0.1665886  0.04565117 0.00364771\n",
      "  0.5        0.11204704]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33403307\n",
      "  0.94479948 0.00151625 0.00482072 0.55477923 0.00196558 0.00910306\n",
      "  0.90114391 0.00436397 0.98507971 0.97916371 0.00477298 0.00290041\n",
      "  0.00776757 0.00423554 0.00928525 0.06416387 0.93761414 0.00753973\n",
      "  0.26894143 0.31864628]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "dataset = shuffle(dataset, random_state=111)\n",
    "print(dataset.shape)\n",
    "print(dataset[0,:2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, random_state=10, shuffle=True)\n",
    "for train, test in kf.split(dataset):\n",
    "    print(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Hyper Parameters\n",
      "\n",
      "Learning Rate: 0.0005\n",
      "Drop out: 0.3\n",
      "Batch dim: 64\n",
      "Number of epochs: 10\n",
      "Regularizers: 1e-04\n",
      "\n",
      "Loss: categorical_crossentropy\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 19, 16)            1024      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 19, 16)            64        \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 19, 16)            784       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 19, 16)            64        \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 19, 16)            1296      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 19, 16)            64        \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 19, 16)            1808      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 19, 16)            64        \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 19, 16)            784       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 19, 16)            64        \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 19, 16)            1296      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 19, 16)            64        \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 19, 16)            1808      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 19, 16)            64        \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 19, 16)            784       \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 19, 16)            64        \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 19, 16)            1296      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 19, 16)            64        \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 19, 16)            1808      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 19, 16)            64        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 304)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               61000     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 1608      \n",
      "=================================================================\n",
      "Total params: 75,936\n",
      "Trainable params: 75,616\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "Train on 1020152 samples, validate on 113350 samples\n",
      "Epoch 1/10\n",
      "  97728/1020152 [=>............................] - ETA: 14:21 - loss: 1.6869 - accuracy: 0.3592 - mae: 0.1872"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-9bd500cff993>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     history = m.fit(dataset[r:n][:,:,0:21], dataset[r:n][:, 0, 21 : (21+classSize)],\n\u001b[1;32m---> 16\u001b[1;33m                     epochs=nn_epochs, validation_data=(dataset[l:r][:,:,0:21], dataset[l:r][:, 0, 21 : (21+classSize)]) ,shuffle=True,batch_size=batch_dim)\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\\nTime elapsed: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"{0:.2f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" s\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3733\u001b[0m     return nest.pack_sequence_as(\n\u001b[0;32m   3734\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3735\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3736\u001b[0m         expand_composites=True)\n\u001b[0;32m   3737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3733\u001b[0m     return nest.pack_sequence_as(\n\u001b[0;32m   3734\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3735\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3736\u001b[0m         expand_composites=True)\n\u001b[0;32m   3737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "l = 0\n",
    "r = 113350\n",
    "n = 1133502\n",
    "\n",
    "fold = 0\n",
    "valid_indexes = [()]\n",
    "\n",
    "\n",
    "for i in range (10):\n",
    "  \n",
    "    fold += 1\n",
    "    print(fold)\n",
    "    \n",
    "    m = get_model()\n",
    "    start_time = timer()\n",
    "    history = m.fit(np.concatenate((dataset[0:l], dataset[r:n]), axis=0)[:,:,0:21], np.concatenate((dataset[0:l], dataset[r:n]), axis=0)[:, 0, 21 : (21+classSize)],\n",
    "                    epochs=nn_epochs, validation_data=(dataset[l:r][:,:,0:21], dataset[l:r][:, 0, 21 : (21+classSize)]) ,shuffle=True,batch_size=batch_dim)\n",
    "    end_time = timer()\n",
    "    print(\"\\n\\nTime elapsed: \" + \"{0:.2f}\".format((end_time - start_time)) + \" s\")\n",
    "    \n",
    "    scores = m.evaluate(dataset[l:r][:,:,0:21], dataset[l:r][:, 0, 21 : (21+classSize)])\n",
    "    print(\"Loss: \" + str(scores[0]) + \", Accuracy: \" + str(scores[1]) + \", MAE: \" + str(scores[2]) + ' fold ' + str(fold))\n",
    "    \n",
    "    \n",
    "    m.save('model_kfold' + str(fold) + '.h5')  # creates a HDF5 file \n",
    "    with open('model_scores_kfold' + str(fold), 'wb') as file_pi:\n",
    "        pickle.dump(scores, file_pi)\n",
    "    with open('model_history_kfold' + str(fold), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "    \n",
    "    valid_indexes.append((l, r))\n",
    "    r += 113350\n",
    "    l += 113350\n",
    "\n",
    "with open('model_kfold_indexes', 'wb') as file_pi:\n",
    "    pickle.dump(valid_indexes, file_pi)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, random_state=10, shuffle=True)\n",
    "fold = 0\n",
    "valid_indexes = [([],[])]\n",
    "for train, test in kf.split(dataset):\n",
    "\n",
    "    fold += 1\n",
    "    print(fold)\n",
    "    \n",
    "    m = get_model()\n",
    "    start_time = timer()\n",
    "    history = m.fit(dataset[train][:,:,0:21], dataset[train][:,:,21 : (21+classSize)], epochs=nn_epochs,\n",
    "                    batch_size=batch_dim, validation_data=(dataset[test][:,:,0:21], dataset[test][:,:,21 : (21+classSize)]),shuffle=True)\n",
    "    end_time = timer()\n",
    "    print(\"\\n\\nTime elapsed: \" + \"{0:.2f}\".format((end_time - start_time)) + \" s\")\n",
    "\n",
    "    \n",
    "    scores = m.evaluate(dataset[test][:,:,0:21], dataset[test][:,:,21 : (21+classSize)])\n",
    "    print(\"Loss: \" + str(scores[0]) + \", Accuracy: \" + str(scores[1]) + \", MAE: \" + str(scores[2]) + ' fold ' + str(fold))\n",
    "    \n",
    "    \n",
    "    m.save('model_kfold' + str(fold) + '.h5')  # creates a HDF5 file \n",
    "    with open('model_scores_kfold' + str(fold), 'wb') as file_pi:\n",
    "        pickle.dump(scores, file_pi)\n",
    "    with open('model_history_kfold' + str(fold), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "    \n",
    "    valid_indexes.append((train, test))\n",
    "\n",
    "with open('model_kfold_indexes', 'wb') as file_pi:\n",
    "    pickle.dump(valid_indexes, file_pi)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
